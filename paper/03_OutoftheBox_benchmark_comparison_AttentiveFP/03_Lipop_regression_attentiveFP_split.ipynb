{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molmap import dataset\n",
    "from molmap import loadmap\n",
    "import molmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "tqdm.pandas(ascii=True)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import MaxPool2D, GlobalMaxPool2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Conv2D, Concatenate,Flatten, Dense, Dropout\n",
    "\n",
    "\n",
    "#use the second GPU, if negative value, CPUs will be used\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "def get_deepchem_idx(df):\n",
    "    \"\"\" deepchem dataset\"\"\"\n",
    "    deepchem_data_name = './Lipop_deepchem.data'\n",
    "    if os.path.exists(deepchem_data_name):\n",
    "        train_df,valid_df,test_df = load(deepchem_data_name)\n",
    "    else:\n",
    "        import deepchem as dc\n",
    "        task, train_valid_test, _ = dc.molnet.load_lipo(featurizer='Raw',split = 'random')\n",
    "        train, valid, test = train_valid_test\n",
    "        print('training set: %s, valid set: %s, test set %s' % (len(train.ids), len(valid.ids), len(test.ids)))\n",
    "        train_df = df[df.smiles.isin(train.ids)]\n",
    "        valid_df = df[df.smiles.isin(valid.ids)]\n",
    "        test_df = df[df.smiles.isin(test.ids)]\n",
    "        dump((train_df,valid_df,test_df), deepchem_data_name)\n",
    "    train_idx = train_df.index\n",
    "    valid_idx = valid_df.index\n",
    "    test_idx = test_df.index\n",
    "    print('training set: %s, valid set: %s, test set %s' % (len(train_idx), len(valid_idx), len(test_idx)))\n",
    "    return train_idx, valid_idx, test_idx\n",
    "\n",
    "\n",
    "def get_attentiveFP_idx(df):\n",
    "    \"\"\" attentiveFP dataset\"\"\"\n",
    "    train, valid,test = load('./Lipop_attentiveFP.data')\n",
    "    print('training set: %s, valid set: %s, test set %s' % (len(train), len(valid), len(test)))\n",
    "    train_idx = df[df.smiles.isin(train.smiles)].index\n",
    "    valid_idx = df[df.smiles.isin(valid.smiles)].index\n",
    "    test_idx = df[df.smiles.isin(test.smiles)].index\n",
    "    print('training set: %s, valid set: %s, test set %s' % (len(train_idx), len(valid_idx), len(test_idx)))\n",
    "    return train_idx, valid_idx, test_idx \n",
    "\n",
    "\n",
    "def Inception(inputs, units = 8, strides = 1):\n",
    "    \"\"\"\n",
    "    naive google inception block\n",
    "    \"\"\"\n",
    "    x1 = Conv2D(units, 5, padding='same', activation = 'relu', strides = strides)(inputs)\n",
    "    x2 = Conv2D(units, 3, padding='same', activation = 'relu', strides = strides)(inputs)\n",
    "    x3 = Conv2D(units, 1, padding='same', activation = 'relu', strides = strides)(inputs)\n",
    "    outputs = Concatenate()([x1, x2, x3])    \n",
    "    return outputs\n",
    "\n",
    "    \n",
    "def SinglePathClassificationModel(molmap_shape,  n_outputs = 1, strides = 1):\n",
    "    \"\"\"molmap_shape: w, h, c\"\"\"\n",
    "    \n",
    "    assert len(molmap_shape) == 3\n",
    "    inputs = Input(molmap_shape)\n",
    "    \n",
    "    conv1 = Conv2D(48, 13, padding = 'same', activation='relu', strides = 1)(inputs)\n",
    "    \n",
    "    conv1 = MaxPool2D(pool_size = 3, strides = 2, padding = 'same')(conv1) #p1\n",
    "    \n",
    "    incept1 = Inception(conv1, strides = 1, units = 32)\n",
    "    \n",
    "    incept1 = MaxPool2D(pool_size = 3, strides = 2, padding = 'same')(incept1) #p2\n",
    "    \n",
    "    incept2 = Inception(incept1, strides = 1, units = 64)\n",
    "    \n",
    "    #flatten\n",
    "    flat1 = GlobalMaxPool2D()(incept2)   \n",
    "    d1 = Dense(128,activation='relu')(flat1)\n",
    "    d1 = Dense(64,activation='relu')(d1)\n",
    "    outputs = Dense(n_outputs,activation='linear')(d1)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def DoublePathClassificationModel(molmap1_size, molmap2_size, n_outputs = 1):\n",
    "    \n",
    "    ## first inputs\n",
    "    d_inputs1 = Input(molmap1_size)\n",
    "    d_conv1 = Conv2D(48, 13, padding = 'same', activation='relu', strides = 1)(d_inputs1)\n",
    "    d_pool1 = MaxPool2D(pool_size = 3, strides = 2, padding = 'same')(d_conv1) #p1\n",
    "    d_incept1 = Inception(d_pool1, strides = 1, units = 32)\n",
    "    d_pool2 = MaxPool2D(pool_size = 3, strides = 2, padding = 'same')(d_incept1) #p2\n",
    "    d_incept2 = Inception(d_pool2, strides = 1, units = 64)\n",
    "    d_flat1 = GlobalMaxPool2D()(d_incept2)\n",
    "\n",
    "    \n",
    "    ## second inputs\n",
    "    f_inputs1 = Input(molmap2_size)\n",
    "    f_conv1 = Conv2D(48, 13, padding = 'same', activation='relu', strides = 1)(f_inputs1)\n",
    "    f_pool1 = MaxPool2D(pool_size = 3, strides = 2, padding = 'same')(f_conv1) #p1\n",
    "    f_incept1 = Inception(f_pool1, strides = 1, units = 32)\n",
    "    f_pool2 = MaxPool2D(pool_size = 3, strides = 2, padding = 'same')(f_incept1) #p2\n",
    "    f_incept2 = Inception(f_pool2, strides = 1, units = 64)\n",
    "    f_flat1 = GlobalMaxPool2D()(f_incept2)    \n",
    "    \n",
    "    ## concat\n",
    "    merge = Concatenate()([d_flat1, f_flat1]) \n",
    "    d1 = Dense(256,activation='relu')(merge)\n",
    "    d1 = Dense(128,activation= 'relu')(d1)\n",
    "    d1 = Dense(32,activation= 'relu')(d1)\n",
    "\n",
    "\n",
    "    outputs = Dense(n_outputs, activation='linear')(d1)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[d_inputs1, f_inputs1], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples: 4200\n",
      "training set: 3360, valid set: 420, test set 420\n",
      "training set: 3360, valid set: 420, test set 420\n",
      "3360 420 420\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from cbks import RegressionPerformance, EarlyStoppingAtMinLoss\n",
    "\n",
    "#load dataset\n",
    "data = dataset.load_Lipop()\n",
    "df = data.data\n",
    "Y = data.y\n",
    "\n",
    "# featurelizer by molmap\n",
    "X1_name =  './descriptor_grid_split.data'\n",
    "X2_name =  './fingerprint_grid_split.data'\n",
    "\n",
    "if os.path.exists(X1_name):\n",
    "    X1 = load(X1_name)\n",
    "else:\n",
    "    #mp1 = molmap.MolMap(ftype = 'descriptor', fmap_type = 'grid', split_channels=True)\n",
    "    #mp1.fit(method = 'umap', min_dist = 0.1, n_neighbors = 50)\n",
    "    #mp1.save('../descriptor_grid_split.mp')\n",
    "    mp1 = loadmap('../../descriptor_grid_split.mp')\n",
    "    X1 = mp1.batch_transform(data.x, n_jobs = 8)\n",
    "    dump(X1, X1_name)\n",
    "\n",
    "if os.path.exists(X2_name):\n",
    "    X2 = load(X2_name)\n",
    "else:\n",
    "    #mp2 = molmap.MolMap(ftype = 'fingerprint', flist = flist, fmap_type = 'grid', split_channels=True)\n",
    "    #mp2.fit(method = 'umap')\n",
    "    #mp2.save('../fingerprint_grid_split.mp')\n",
    "    mp2 = loadmap('../../fingerprint_grid_split.mp')\n",
    "    X2 = mp2.batch_transform(data.x, n_jobs = 8)\n",
    "    dump(X2, X2_name)\n",
    "\n",
    "epochs = 500\n",
    "patience = 20\n",
    "batch_size = 128\n",
    "learning_rate=0.0001\n",
    "\n",
    "seed = 77\n",
    "dtype = 'attentiveFP'\n",
    "mtype = 'both'\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)                \n",
    "\n",
    "## train, valid, test split                \n",
    "if dtype == 'attentiveFP':\n",
    "    train_idx, valid_idx, test_idx = get_attentiveFP_idx(df) #random seed has no effects\n",
    "else:\n",
    "    train_idx, valid_idx, test_idx = get_deepchem_idx(df) #random seed has no effects              \n",
    "\n",
    "trainY = Y[train_idx]\n",
    "validY = Y[valid_idx]\n",
    "testY = Y[test_idx]\n",
    "\n",
    "print(len(train_idx), len(valid_idx), len(test_idx))    \n",
    "molmap1_size = X1.shape[1:]\n",
    "molmap2_size = X2.shape[1:]\n",
    "\n",
    "if mtype == 'SinglePath_descriptor':\n",
    "    model = SinglePathClassificationModel(molmap1_size, n_outputs = Y.shape[1])\n",
    "    trainX = X1[train_idx]                \n",
    "    validX = X1[valid_idx]\n",
    "    testX = X1[test_idx]\n",
    "\n",
    "elif mtype == 'SinglePath_fingerprint':\n",
    "    model = SinglePathClassificationModel(molmap2_size, n_outputs = Y.shape[1])\n",
    "    trainX = X2[train_idx]                \n",
    "    validX = X2[valid_idx]\n",
    "    testX = X2[test_idx]                \n",
    "\n",
    "else:\n",
    "    model = DoublePathClassificationModel(molmap1_size, molmap2_size, n_outputs = Y.shape[1])\n",
    "    trainX = (X1[train_idx], X2[train_idx])    \n",
    "    validX = (X1[valid_idx], X2[valid_idx])                    \n",
    "    testX = (X1[test_idx], X2[test_idx])                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0001, loss: 2.8032 - val_loss: 1.5310; rmse: 1.1832 - rmse_val: 1.2373;  r2: 0.0158 - r2_val: 0.0166                                                                                                    \n",
      "epoch: 0002, loss: 1.4053 - val_loss: 1.4848; rmse: 1.1687 - rmse_val: 1.2185;  r2: 0.0397 - r2_val: 0.0462                                                                                                    \n",
      "epoch: 0003, loss: 1.3348 - val_loss: 1.4258; rmse: 1.1400 - rmse_val: 1.1941;  r2: 0.0863 - r2_val: 0.0841                                                                                                    \n",
      "epoch: 0004, loss: 1.2771 - val_loss: 1.3577; rmse: 1.1114 - rmse_val: 1.1652;  r2: 0.1315 - r2_val: 0.1279                                                                                                    \n",
      "epoch: 0005, loss: 1.2075 - val_loss: 1.2647; rmse: 1.0754 - rmse_val: 1.1246;  r2: 0.1870 - r2_val: 0.1876                                                                                                    \n",
      "epoch: 0006, loss: 1.1121 - val_loss: 1.1854; rmse: 1.0484 - rmse_val: 1.0887;  r2: 0.2273 - r2_val: 0.2386                                                                                                    \n",
      "epoch: 0007, loss: 1.0483 - val_loss: 1.0154; rmse: 0.9842 - rmse_val: 1.0077;  r2: 0.3190 - r2_val: 0.3478                                                                                                    \n",
      "epoch: 0008, loss: 0.9626 - val_loss: 1.0277; rmse: 0.9989 - rmse_val: 1.0138;  r2: 0.2985 - r2_val: 0.3398                                                                                                    \n",
      "epoch: 0009, loss: 0.9209 - val_loss: 0.8471; rmse: 0.9107 - rmse_val: 0.9204;  r2: 0.4169 - r2_val: 0.4559                                                                                                    \n",
      "epoch: 0010, loss: 0.8321 - val_loss: 0.8670; rmse: 0.9186 - rmse_val: 0.9311;  r2: 0.4068 - r2_val: 0.4431                                                                                                    \n",
      "epoch: 0011, loss: 0.7772 - val_loss: 0.7440; rmse: 0.8490 - rmse_val: 0.8626;  r2: 0.4932 - r2_val: 0.5221                                                                                                    \n",
      "epoch: 0012, loss: 0.7144 - val_loss: 0.7037; rmse: 0.8191 - rmse_val: 0.8389;  r2: 0.5283 - r2_val: 0.5480                                                                                                    \n",
      "epoch: 0013, loss: 0.6663 - val_loss: 0.6759; rmse: 0.7939 - rmse_val: 0.8221;  r2: 0.5569 - r2_val: 0.5659                                                                                                    \n",
      "epoch: 0014, loss: 0.6403 - val_loss: 0.6884; rmse: 0.7927 - rmse_val: 0.8297;  r2: 0.5582 - r2_val: 0.5578                                                                                                    \n",
      "epoch: 0015, loss: 0.5882 - val_loss: 0.6462; rmse: 0.7655 - rmse_val: 0.8038;  r2: 0.5880 - r2_val: 0.5849                                                                                                    \n",
      "epoch: 0016, loss: 0.5617 - val_loss: 0.6225; rmse: 0.7400 - rmse_val: 0.7890;  r2: 0.6151 - r2_val: 0.6001                                                                                                    \n",
      "epoch: 0017, loss: 0.5366 - val_loss: 0.6214; rmse: 0.7381 - rmse_val: 0.7883;  r2: 0.6170 - r2_val: 0.6009                                                                                                    \n",
      "epoch: 0018, loss: 0.5129 - val_loss: 0.5634; rmse: 0.6869 - rmse_val: 0.7506;  r2: 0.6683 - r2_val: 0.6381                                                                                                    \n",
      "epoch: 0019, loss: 0.4710 - val_loss: 0.5780; rmse: 0.6894 - rmse_val: 0.7603;  r2: 0.6658 - r2_val: 0.6287                                                                                                    \n",
      "epoch: 0020, loss: 0.4399 - val_loss: 0.5459; rmse: 0.6659 - rmse_val: 0.7389;  r2: 0.6882 - r2_val: 0.6493                                                                                                    \n",
      "epoch: 0021, loss: 0.4511 - val_loss: 0.5521; rmse: 0.6645 - rmse_val: 0.7431;  r2: 0.6896 - r2_val: 0.6453                                                                                                    \n",
      "epoch: 0022, loss: 0.3984 - val_loss: 0.5147; rmse: 0.6270 - rmse_val: 0.7174;  r2: 0.7236 - r2_val: 0.6694                                                                                                    \n",
      "epoch: 0023, loss: 0.3943 - val_loss: 0.4914; rmse: 0.6004 - rmse_val: 0.7010;  r2: 0.7466 - r2_val: 0.6844                                                                                                    \n",
      "epoch: 0024, loss: 0.3847 - val_loss: 0.4952; rmse: 0.6009 - rmse_val: 0.7037;  r2: 0.7461 - r2_val: 0.6819                                                                                                    \n",
      "epoch: 0025, loss: 0.3424 - val_loss: 0.4552; rmse: 0.5605 - rmse_val: 0.6747;  r2: 0.7792 - r2_val: 0.7076                                                                                                    \n",
      "epoch: 0026, loss: 0.3295 - val_loss: 0.4833; rmse: 0.5758 - rmse_val: 0.6952;  r2: 0.7669 - r2_val: 0.6895                                                                                                    \n",
      "epoch: 0027, loss: 0.3260 - val_loss: 0.4399; rmse: 0.5345 - rmse_val: 0.6632;  r2: 0.7991 - r2_val: 0.7174                                                                                                    \n",
      "epoch: 0028, loss: 0.2984 - val_loss: 0.4326; rmse: 0.5163 - rmse_val: 0.6577;  r2: 0.8126 - r2_val: 0.7221                                                                                                    \n",
      "epoch: 0029, loss: 0.3017 - val_loss: 0.4965; rmse: 0.5726 - rmse_val: 0.7046;  r2: 0.7695 - r2_val: 0.6811                                                                                                    \n",
      "epoch: 0030, loss: 0.2929 - val_loss: 0.4440; rmse: 0.5112 - rmse_val: 0.6663;  r2: 0.8163 - r2_val: 0.7148                                                                                                    \n",
      "epoch: 0031, loss: 0.2615 - val_loss: 0.4487; rmse: 0.5121 - rmse_val: 0.6699;  r2: 0.8156 - r2_val: 0.7117                                                                                                    \n",
      "epoch: 0032, loss: 0.2422 - val_loss: 0.4119; rmse: 0.4641 - rmse_val: 0.6418;  r2: 0.8486 - r2_val: 0.7354                                                                                                    \n",
      "epoch: 0033, loss: 0.2294 - val_loss: 0.4165; rmse: 0.4583 - rmse_val: 0.6453;  r2: 0.8524 - r2_val: 0.7325                                                                                                    \n",
      "epoch: 0034, loss: 0.2251 - val_loss: 0.4060; rmse: 0.4404 - rmse_val: 0.6372;  r2: 0.8636 - r2_val: 0.7392                                                                                                    \n",
      "epoch: 0035, loss: 0.2114 - val_loss: 0.4293; rmse: 0.4626 - rmse_val: 0.6552;  r2: 0.8496 - r2_val: 0.7242                                                                                                    \n",
      "epoch: 0036, loss: 0.1927 - val_loss: 0.3948; rmse: 0.4154 - rmse_val: 0.6283;  r2: 0.8787 - r2_val: 0.7464                                                                                                    \n",
      "epoch: 0037, loss: 0.2055 - val_loss: 0.5018; rmse: 0.5164 - rmse_val: 0.7083;  r2: 0.8125 - r2_val: 0.6777                                                                                                    \n",
      "epoch: 0038, loss: 0.1955 - val_loss: 0.4231; rmse: 0.4374 - rmse_val: 0.6504;  r2: 0.8655 - r2_val: 0.7282                                                                                                    \n",
      "epoch: 0039, loss: 0.1823 - val_loss: 0.3996; rmse: 0.4021 - rmse_val: 0.6321;  r2: 0.8863 - r2_val: 0.7433                                                                                                    \n",
      "epoch: 0040, loss: 0.1662 - val_loss: 0.3857; rmse: 0.3701 - rmse_val: 0.6210;  r2: 0.9037 - r2_val: 0.7523                                                                                                    \n",
      "epoch: 0041, loss: 0.1564 - val_loss: 0.3906; rmse: 0.3679 - rmse_val: 0.6250;  r2: 0.9049 - r2_val: 0.7491                                                                                                    \n",
      "epoch: 0042, loss: 0.1855 - val_loss: 0.3928; rmse: 0.3727 - rmse_val: 0.6267;  r2: 0.9024 - r2_val: 0.7477                                                                                                    \n",
      "epoch: 0043, loss: 0.1433 - val_loss: 0.3813; rmse: 0.3496 - rmse_val: 0.6175;  r2: 0.9141 - r2_val: 0.7551                                                                                                    \n",
      "epoch: 0044, loss: 0.1570 - val_loss: 0.4374; rmse: 0.4116 - rmse_val: 0.6613;  r2: 0.8809 - r2_val: 0.7190                                                                                                    \n",
      "epoch: 0045, loss: 0.1410 - val_loss: 0.4280; rmse: 0.4013 - rmse_val: 0.6542;  r2: 0.8868 - r2_val: 0.7251                                                                                                    \n",
      "epoch: 0046, loss: 0.1262 - val_loss: 0.3928; rmse: 0.3356 - rmse_val: 0.6267;  r2: 0.9208 - r2_val: 0.7477                                                                                                    \n",
      "epoch: 0047, loss: 0.1119 - val_loss: 0.4069; rmse: 0.3570 - rmse_val: 0.6379;  r2: 0.9104 - r2_val: 0.7386                                                                                                    \n",
      "epoch: 0048, loss: 0.1139 - val_loss: 0.4314; rmse: 0.3772 - rmse_val: 0.6568;  r2: 0.9000 - r2_val: 0.7229                                                                                                    \n",
      "epoch: 0049, loss: 0.1212 - val_loss: 0.3823; rmse: 0.3072 - rmse_val: 0.6183;  r2: 0.9337 - r2_val: 0.7544                                                                                                    \n",
      "epoch: 0050, loss: 0.0961 - val_loss: 0.3799; rmse: 0.2986 - rmse_val: 0.6164;  r2: 0.9373 - r2_val: 0.7560                                                                                                    \n",
      "epoch: 0051, loss: 0.0893 - val_loss: 0.3801; rmse: 0.2874 - rmse_val: 0.6165;  r2: 0.9419 - r2_val: 0.7558                                                                                                    \n",
      "epoch: 0052, loss: 0.0873 - val_loss: 0.3740; rmse: 0.2719 - rmse_val: 0.6115;  r2: 0.9480 - r2_val: 0.7598                                                                                                    \n",
      "epoch: 0053, loss: 0.0876 - val_loss: 0.3904; rmse: 0.2893 - rmse_val: 0.6248;  r2: 0.9412 - r2_val: 0.7493                                                                                                    \n",
      "epoch: 0054, loss: 0.0889 - val_loss: 0.3715; rmse: 0.2579 - rmse_val: 0.6095;  r2: 0.9532 - r2_val: 0.7613                                                                                                    \n",
      "epoch: 0055, loss: 0.0746 - val_loss: 0.3837; rmse: 0.2745 - rmse_val: 0.6195;  r2: 0.9470 - r2_val: 0.7535                                                                                                    \n",
      "epoch: 0056, loss: 0.0846 - val_loss: 0.3956; rmse: 0.3008 - rmse_val: 0.6290;  r2: 0.9364 - r2_val: 0.7459                                                                                                    \n",
      "epoch: 0057, loss: 0.0729 - val_loss: 0.3731; rmse: 0.2415 - rmse_val: 0.6108;  r2: 0.9590 - r2_val: 0.7603                                                                                                    \n",
      "epoch: 0058, loss: 0.0834 - val_loss: 0.3913; rmse: 0.2668 - rmse_val: 0.6255;  r2: 0.9500 - r2_val: 0.7487                                                                                                    \n",
      "epoch: 0059, loss: 0.0905 - val_loss: 0.4076; rmse: 0.2951 - rmse_val: 0.6384;  r2: 0.9388 - r2_val: 0.7382                                                                                                    \n",
      "epoch: 0060, loss: 0.0702 - val_loss: 0.3918; rmse: 0.2660 - rmse_val: 0.6259;  r2: 0.9503 - r2_val: 0.7483                                                                                                    \n",
      "epoch: 0061, loss: 0.0739 - val_loss: 0.4642; rmse: 0.3779 - rmse_val: 0.6813;  r2: 0.8996 - r2_val: 0.7018                                                                                                    \n",
      "epoch: 0062, loss: 0.0981 - val_loss: 0.3766; rmse: 0.2423 - rmse_val: 0.6136;  r2: 0.9587 - r2_val: 0.7581                                                                                                    \n",
      "epoch: 0063, loss: 0.0617 - val_loss: 0.3831; rmse: 0.2386 - rmse_val: 0.6190;  r2: 0.9600 - r2_val: 0.7539                                                                                                    \n",
      "epoch: 0064, loss: 0.0606 - val_loss: 0.4633; rmse: 0.3611 - rmse_val: 0.6807;  r2: 0.9083 - r2_val: 0.7024                                                                                                    \n",
      "epoch: 0065, loss: 0.0592 - val_loss: 0.3933; rmse: 0.2461 - rmse_val: 0.6271;  r2: 0.9574 - r2_val: 0.7474                                                                                                    \n",
      "epoch: 0066, loss: 0.0647 - val_loss: 0.3731; rmse: 0.2015 - rmse_val: 0.6108;  r2: 0.9715 - r2_val: 0.7603                                                                                                    \n",
      "epoch: 0067, loss: 0.0528 - val_loss: 0.3743; rmse: 0.2025 - rmse_val: 0.6118;  r2: 0.9712 - r2_val: 0.7596                                                                                                    \n",
      "epoch: 0068, loss: 0.0444 - val_loss: 0.4348; rmse: 0.3146 - rmse_val: 0.6594;  r2: 0.9304 - r2_val: 0.7207                                                                                                    \n",
      "epoch: 0069, loss: 0.0577 - val_loss: 0.4050; rmse: 0.2492 - rmse_val: 0.6364;  r2: 0.9564 - r2_val: 0.7399                                                                                                    \n",
      "epoch: 0070, loss: 0.0591 - val_loss: 0.3775; rmse: 0.2043 - rmse_val: 0.6144;  r2: 0.9707 - r2_val: 0.7575                                                                                                    \n",
      "epoch: 0071, loss: 0.0457 - val_loss: 0.3739; rmse: 0.1805 - rmse_val: 0.6115;  r2: 0.9771 - r2_val: 0.7598                                                                                                    \n",
      "epoch: 0072, loss: 0.0366 - val_loss: 0.3715; rmse: 0.1726 - rmse_val: 0.6095;  r2: 0.9791 - r2_val: 0.7614                                                                                                    \n",
      "epoch: 0073, loss: 0.0374 - val_loss: 0.3907; rmse: 0.2168 - rmse_val: 0.6251;  r2: 0.9669 - r2_val: 0.7490                                                                                                    \n",
      "epoch: 0074, loss: 0.0439 - val_loss: 0.3721; rmse: 0.1663 - rmse_val: 0.6100;  r2: 0.9806 - r2_val: 0.7610                                                                                                    \n",
      "epoch: 0075, loss: 0.0300 - val_loss: 0.3745; rmse: 0.1635 - rmse_val: 0.6120;  r2: 0.9812 - r2_val: 0.7594                                                                                                    \n",
      "epoch: 0076, loss: 0.0318 - val_loss: 0.3798; rmse: 0.1731 - rmse_val: 0.6163;  r2: 0.9789 - r2_val: 0.7561                                                                                                    \n",
      "epoch: 0077, loss: 0.0289 - val_loss: 0.3792; rmse: 0.1681 - rmse_val: 0.6158;  r2: 0.9801 - r2_val: 0.7564                                                                                                    \n",
      "epoch: 0078, loss: 0.0323 - val_loss: 0.3859; rmse: 0.1819 - rmse_val: 0.6212;  r2: 0.9767 - r2_val: 0.7521                                                                                                    \n",
      "epoch: 0079, loss: 0.0291 - val_loss: 0.3743; rmse: 0.1452 - rmse_val: 0.6118;  r2: 0.9852 - r2_val: 0.7595                                                                                                    \n",
      "epoch: 0080, loss: 0.0297 - val_loss: 0.3946; rmse: 0.2002 - rmse_val: 0.6282;  r2: 0.9718 - r2_val: 0.7465                                                                                                    \n",
      "epoch: 0081, loss: 0.0313 - val_loss: 0.3885; rmse: 0.1753 - rmse_val: 0.6233;  r2: 0.9784 - r2_val: 0.7504                                                                                                    \n",
      "epoch: 0082, loss: 0.0279 - val_loss: 0.3751; rmse: 0.1395 - rmse_val: 0.6124;  r2: 0.9863 - r2_val: 0.7591                                                                                                    \n",
      "epoch: 0083, loss: 0.0345 - val_loss: 0.4054; rmse: 0.2224 - rmse_val: 0.6367;  r2: 0.9652 - r2_val: 0.7396                                                                                                    \n",
      "epoch: 0084, loss: 0.0292 - val_loss: 0.3768; rmse: 0.1348 - rmse_val: 0.6139;  r2: 0.9872 - r2_val: 0.7579                                                                                                    \n",
      "epoch: 0085, loss: 0.0246 - val_loss: 0.3865; rmse: 0.1618 - rmse_val: 0.6217;  r2: 0.9816 - r2_val: 0.7517                                                                                                    \n",
      "epoch: 0086, loss: 0.0244 - val_loss: 0.3765; rmse: 0.1294 - rmse_val: 0.6136;  r2: 0.9882 - r2_val: 0.7582                                                                                                    \n",
      "epoch: 0087, loss: 0.0210 - val_loss: 0.3784; rmse: 0.1251 - rmse_val: 0.6151;  r2: 0.9890 - r2_val: 0.7569                                                                                                    \n",
      "epoch: 0088, loss: 0.0296 - val_loss: 0.3941; rmse: 0.1909 - rmse_val: 0.6278;  r2: 0.9744 - r2_val: 0.7468                                                                                                    \n",
      "epoch: 0089, loss: 0.0271 - val_loss: 0.3806; rmse: 0.1336 - rmse_val: 0.6169;  r2: 0.9875 - r2_val: 0.7555                                                                                                    \n",
      "epoch: 0090, loss: 0.0363 - val_loss: 0.4131; rmse: 0.2210 - rmse_val: 0.6427;  r2: 0.9657 - r2_val: 0.7346                                                                                                    \n",
      "epoch: 0091, loss: 0.0270 - val_loss: 0.3802; rmse: 0.1226 - rmse_val: 0.6166;  r2: 0.9894 - r2_val: 0.7558                                                                                                    \n",
      "epoch: 0092, loss: 0.0195 - val_loss: 0.3803; rmse: 0.1217 - rmse_val: 0.6167;  r2: 0.9896 - r2_val: 0.7557                                                                                                    \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00092: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f088e102278>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "earlystop = EarlyStoppingAtMinLoss(patience=patience, criteria = 'val_loss')\n",
    "performace = RegressionPerformance((trainX, trainY), (validX, validY))\n",
    "lr = tf.keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #\n",
    "model.compile(optimizer = lr, loss = 'mse')\n",
    "\n",
    "model.fit(trainX, trainY, batch_size=batch_size, \n",
    "          epochs=epochs, verbose= 0, shuffle = True, \n",
    "          validation_data = (validX, validY), \n",
    "          callbacks=[performace, earlystop]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f08391b7b70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXwV1dnA8d+5S/aVJCSQhH1HDAFkkU3BBZAqVgRxBaVUqnV5rdYub1utvrVq3apCRbFi3RDRWpUqIIrIvu8QAgECIWQj+3KX8/5xQthCEsiFyfJ8P598yJ2ZO3Pu5PLMmeecOUdprRFCCNH42awugBBCCN+QgC6EEE2EBHQhhGgiJKALIUQTIQFdCCGaCIdVB46Ojtbt2rWz6vBCCNEorVu3LltrHVPdOssCert27Vi7dq1VhxdCiEZJKbX/bOsk5SKEEE2EBHQhhGgiJKALIUQTYVkOXQhx8bhcLtLT0ykrK7O6KKKOAgICSEhIwOl01vk9EtCFaAbS09MJDQ2lXbt2KKWsLo6ohdaanJwc0tPTad++fZ3fJykXIZqBsrIyoqKiJJg3EkopoqKizvmOSgK6EM2EBPPG5Xz+XpYF9KOF5VYdWgghmiQLA7o0zgghhC9ZFtC1Nol/IUTTd+zYMV5//fVzft+YMWM4duxYjdv84Q9/YNGiRedbtGqFhIT4dH8Xi6U59HK318rDCyEukrMFdLfbXeP7vvrqKyIiImrc5sknn+Sqq66qV/maCku7LZa5PAQ47VYWQYhm54n/bGP74QKf7rNH6zD++JOeZ13/+OOPk5qaSu/evXE6nQQEBBAZGcnOnTvZvXs348aN4+DBg5SVlfHggw8ybdo04MSYT0VFRYwePZohQ4awfPly4uPj+fe//01gYCCTJ09m7NixjB8/nnbt2nHXXXfxn//8B5fLxccff0y3bt3Iysri1ltv5fDhwwwaNIiFCxeybt06oqOja/xcWmsee+wxFixYgFKK3//+90ycOJGMjAwmTpxIQUEBbrebGTNmcPnll3PPPfewdu1alFLcfffdPPzwwz49z7WxtIZe6vJYeXghxEXyzDPP0LFjRzZu3Mhzzz3H+vXrefnll9m9ezcAs2fPZt26daxdu5ZXXnmFnJycM/aRkpLCfffdx7Zt24iIiOCTTz6p9ljR0dGsX7+e6dOn8/zzzwPwxBNPMGLECLZt28b48eM5cOBAnco9f/58Nm7cyKZNm1i0aBGPPvooGRkZvP/++1x77bVV63r37s3GjRs5dOgQW7duZcuWLUyZMuU8z9b5s7iGLikXIS62mmrSF0v//v1PeWDmlVde4dNPPwXg4MGDpKSkEBUVdcp72rdvT+/evQHo27cvaWlp1e77pz/9adU28+fPB2DZsmVV+x81ahSRkZF1KueyZcuYNGkSdrud2NhYhg8fzpo1a7jsssu4++67cblcjBs3jt69e9OhQwf27t3LL3/5S6677jquueaaup8QH7G2hl4hNXQhmqPg4OCq37/77jsWLVrEihUr2LRpE8nJydU+UOPv71/1u91uP2v+/fh2NW1TX8OGDWPp0qXEx8czefJk5syZQ2RkJJs2beKKK65g5syZTJ069YIcuyaWBvQytwR0IZqD0NBQCgsLq12Xn59PZGQkQUFB7Ny5k5UrV/r8+IMHD2bu3LkAfPPNN+Tl5dXpfUOHDuWjjz7C4/GQlZXF0qVL6d+/P/v37yc2Npaf/exnTJ06lfXr15OdnY3X6+Wmm27iqaeeYv369T7/HLWxNuUiNXQhmoWoqCgGDx7MJZdcQmBgILGxsVXrRo0axcyZM+nevTtdu3Zl4MCBPj/+H//4RyZNmsS7777LoEGDiIuLIzQ0tNb33XjjjaxYsYKkpCSUUjz77LPExcXxzjvv8Nxzz+F0OgkJCWHOnDkcOnSIKVOm4PWaVPJf/vIXn3+O2iir+oL7t+qsFyxZxohusbVvLISolx07dtC9e3eri2GZ8vJy7HY7DoeDFStWMH36dDZu3Gh1sWpV3d9NKbVOa92vuu2lUVQI0eQdOHCACRMm4PV68fPzY9asWVYX6YKwNKBLo6gQ4mLo3LkzGzZsOGVZTk4OI0eOPGPbxYsXn9HDprGwtoYujaJCCItERUU1irTLuZBui0II0UTIWC5CCNFESA1dCCGaCMsCuk0pymQsFyGE8BkLA7oMziWEOLvjY5IfPnyY8ePHV7vNFVdcwdq1a2vcz0svvURJSUnV67qMsX4uJk+ezLx583y2v/qwLKArpaQfuhCiVq1bt65XwDw9oNdljPXGqtZui0qpRGAOEAto4A2t9cunbXMF8G9gX+Wi+VrrJ2var00hKRchrLDgcTiyxbf7jOsFo5+pcZPHH3+cxMRE7rvvPgD+9Kc/4XA4WLJkCXl5ebhcLp566iluuOGGU96XlpbG2LFj2bp1K6WlpUyZMoVNmzbRrVs3SktLq7abPn06a9asobS0lPHjx/PEE0/wyiuvcPjwYa688kqio6NZsmRJ1Rjr0dHRvPDCC8yePRuAqVOn8tBDD5GWlnbWsddrs3jxYn71q1/hdru57LLLmDFjBv7+/jz++ON8/vnnOBwOrrnmGp5//nk+/vhjnnjiCex2O+Hh4SxduvRcz/oZ6tIP3Q08orVer5QKBdYppRZqrbeftt0PWuuxdT2wkhy6EM3KxIkTeeihh6oC+ty5c/n666954IEHCAsLIzs7m4EDB3L99defdcb7GTNmEBQUxI4dO9i8eTN9+vSpWvf000/TokULPB4PI0eOZPPmzTzwwAO88MILLFmy5IzJLNatW8fbb7/NqlWr0FozYMAAhg8fTmRkJCkpKXzwwQfMmjWLCRMm8Mknn3D77bfX+PnKysqYPHkyixcvpkuXLtx5553MmDGDO+64g08//ZSdO3eilKpK9zz55JN8/fXXxMfH+ywFVGtA11pnABmVvxcqpXYA8cDpAf2cSA5dCIvUUpO+UJKTkzl69CiHDx8mKyuLyMhI4uLiePjhh1m6dCk2m41Dhw6RmZlJXFxctftYunQpDzzwAACXXnopl156adW6uXPn8sYbb+B2u8nIyGD79u2nrD/dsmXLuPHGG6uG8v3pT3/KDz/8wPXXX1/nsddPtmvXLtq3b0+XLl0AuOuuu3jttde4//77CQgI4J577mHs2LGMHWvqvYMHD2by5MlMmDChagz3+jqnHLpSqh2QDKyqZvUgpdQmpdQCpVS1I+grpaYppdYqpda6XS6poQvRzNx8883MmzePjz76iIkTJ/Lee++RlZXFunXr2LhxI7GxsdWOhV6bffv28fzzz7N48WI2b97Mddddd177Oa6uY6/XhcPhYPXq1YwfP54vvviCUaNGATBz5kyeeuopDh48SN++faudpelc1TmgK6VCgE+Ah7TWp09IuB5oq7VOAv4OfFbdPrTWb2it+2mt+wX4+1MqjaJCNCsTJ07kww8/ZN68edx8883k5+fTsmVLnE4nS5YsYf/+/TW+f9iwYbz//vsAbN26lc2bNwNQUFBAcHAw4eHhZGZmsmDBgqr3nG0s9qFDh/LZZ59RUlJCcXExn376KUOHDj3vz9a1a1fS0tLYs2cPAO+++y7Dhw+nqKiI/Px8xowZw4svvsimTZsASE1NZcCAATz55JPExMRw8ODB8z72cXUay0Up5cQE8/e01vNPX39ygNdaf6WUel0pFa21zj77PqFcauhCNCs9e/aksLCQ+Ph4WrVqxW233cZPfvITevXqRb9+/ejWrVuN758+fTpTpkyhe/fudO/enb59+wKQlJREcnIy3bp1IzExkcGDB1e9Z9q0aYwaNYrWrVuzZMmSquV9+vRh8uTJ9O/fHzCNosnJyXVKr1QnICCAt99+m5tvvrmqUfTee+8lNzeXG264gbKyMrTWvPDCCwA8+uijpKSkoLVm5MiRJCUlnddxT1breOjKtE68A+RqrR86yzZxQKbWWiul+gPzMDX2s+48rmNP3X7qK6z4zZmjnQkhfKu5j4feWF2I8dAHA3cAW5RSx4cm+y3QBkBrPRMYD0xXSrmBUuCWmoI5SLdFIYTwtbr0clkGVN+H6MQ2rwKvnsuBlVLSy0UI0Wjcd999/Pjjj6cse/DBB5kyZYpFJTqTZeOhmxq6F631WfucCiF8R/6v1c9rr712UY93PtODWjo4F8gQukJcDAEBAeTk5JxXkBAXn9aanJwcAgICzul9ltXQj1cUylweApx2q4ohRLOQkJBAeno6WVlZVhdF1FFAQAAJCQnn9B4LUy4KL+Zp0aY5TI4QDYfT6aR9+/ZWF0NcYJYOnwvIiItCCOEjlufQZdYiIYTwDUvHQwcoc0tAF0IIX7A+5SI1dCGE8AmpoQshRBPRAHLo0igqhBC+YH3KRR7/F0IIn7C+hi4BXQghfMLCHLr5V2roQgjhG5bX0CWgCyGEb1haQz8+4qIQQoj6syygAwQ67ZJDF0IIH7E0oAc47ZJyEUIIH7E8oEsNXQghfMPigG6jXHLoQgjhE9bm0P2khi6EEL5ibQ3dITl0IYTwFamhCyFEE2FpQPd32KUfuhBC+IjlNXRJuQghhG9YnEO3SUAXQggfsbyGLjl0IYTwjVoDulIqUSm1RCm1XSm1TSn1YDXbKKXUK0qpPUqpzUqpPnU5uDwpKoQQvuOowzZu4BGt9XqlVCiwTim1UGu9/aRtRgOdK38GADMq/62RCehetNZVU9IJIYQ4P7XW0LXWGVrr9ZW/FwI7gPjTNrsBmKONlUCEUqpVbfsOcJrDl7ulp4sQQtTXOeXQlVLtgGRg1Wmr4oGDJ71O58ygj1JqmlJqrVJqbVZWFgEOOwClFZJ2EUKI+qpzQFdKhQCfAA9prQvO52Ba6ze01v201v1iYmII9DMBvcwtAV0IIeqrTgFdKeXEBPP3tNbzq9nkEJB40uuEymU1Op5ykRq6EELUX116uSjgLWCH1vqFs2z2OXBnZW+XgUC+1jqjtn0HOitr6PK0qBBC1FtderkMBu4AtiilNlYu+y3QBkBrPRP4ChgD7AFKgCl1Obi/U1IuQgjhK7UGdK31MqDGPoVaaw3cd64Hr6qhS8pFCCHqzfIZi0Bq6EII4QuWTxINUFohOXQhhKgvy6egA+TxfyGE8IGGUUOXgC6EEPVm7QQXVd0WJaALIUR9NYgaugR0IYSoP0sDutOusCl5sEgIIXzB0oCulCLQKZNcCCGEL1ga0EEmuRBCCF9pEAFdauhCCFF/DSCg2yiXHLoQQtSb5QFdJooWQgjfsDygBzgkhy6EEL5geUCXGroQQviG5QHd32GXfuhCCOEDlgf0QD9JuQghhC9YHtADHDYJ6EII4QOWB3TJoQshhG9YHtDlSVEhhPCNBhLQvZhpSYUQQpyvBhDQTRHK3dLTRQgh6sPygH5iXlFJuwghRH1YHtADjk9y4ZaALoQQ9WF5QJcauhBC+IblAf14Dl2eFhVCiPppAAG9soYuXReFEKJerAvorlLgREAvl4AuhBD1UmtAV0rNVkodVUptPcv6K5RS+UqpjZU/f6jTkbNTIGXhiRy6BHQhhKiXutTQ/wmMqmWbH7TWvSt/nqzTkR3+8P5EWqbOAySHLoQQ9VVrQNdaLwVyfX7k6M7QfhitvnuE++2fUlrh9vkhhBCiOfFVDn2QUmqTUmqBUqrn2TZSSk1TSq1VSq3Nys6BW+dS1n08v3J+TI+tz/qoKEII0Tz5IqCvB9pqrZOAvwOfnW1DrfUbWut+Wut+MTEx4PCj/Ccz+Kf7GnqkzYEVr/mgOEII0TzVO6BrrQu01kWVv38FOJVS0XV9f6Cfgyfdd5IafSV8/TvYdtbrgRBCiBrUO6ArpeKUUqry9/6V+8yp6/uddgXKxucd/gSJ/WH+NDiwsr7FEkKIZqcu3RY/AFYAXZVS6Uqpe5RS9yql7q3cZDywVSm1CXgFuEWfw1i4SikCnXaKvH5wywcQngAf3AI5qefzeYQQotly1LaB1npSLetfBV6tTyECnJWzFgVHwe3zYNYImHsXTF0IzsD67FoIIZoNyx/9B4gIcrLxwDEq3F5o0QFu/AdkboEFj1ldNCGEaDQaREB/5JqubM8o4G/f7DILulwLQ/4H1s+BjR9YWzghhGgkGkRAH9OrFbcNaMM/lu7lu11HzcIrfwdth8CX/wNHd1hbQCGEaAQaREAH+N+xPegaG8ojczeRWVAGdgeMfwv8Qkw+3V1udRGFEKJBazABPcBp59VbkymucPPwRxvxeDWExsENr0H2LljzltVFFEKIBq3BBHSAzrGhPHF9T5an5vD0lzvQWkOXa6DjCPj+r1Di+yFlhBCiqWhQAR1gQr9Epgxux+wf9zHrh71m4dV/hrJ8+OFv1hZOCCEasAYX0JVS/O91Pbju0lb831c7+XRDOsRdAsm3wap/QO5eq4sohBANUoML6AA2m+KFCUkM7NCCRz/ezNLdWXDl78HuhEVPWF08IYRokBpkQAfwd9h5485+dGoZwvR/rWNveShc/gBs/wwOrra6eEII0eA02IAOEBbgZPbky3A6bNz//gbK+t8HIXHw5SPgkQkxhBDiZA06oAO0jgjk+fFJbM8o4OmFB2D0X+HIZlj+itVFE0KIBqXBB3SAq3rEMnVIe95duZ+vvAOg+/Xw3TOQtdvqogkhRIPRKAI6wGOjupGUGMGv520mfdCfzSiMn98PXo/VRRNCiAah0QR0P4eNVyclg4JH/3sEPeoZOLgKVs+yumhCCNEgNJqADpDYIohHru7Cir05fBcwAjpdDYufgGMHrC6aEEJYrlEFdIBbB7SlbVQQf/3vLjxj/gauUhliVwghaIQB3c9h41fXdGXnkUI+3Wc385Du+srqYgkhhOUaXUAHuK5XKy5NCOeFb3bh6jQKMjZC/iGriyWEEJZqlAHdZlM8Probh/PLmF/cyyzc/V9rCyWEEBZrlAEd4PKO0VzRNYanV3vwRLSHXQusLpIQQliq0QZ0gMdHd6O4wssib1/0vu+hvMjqIgkhhGUadUDvFhfGH3/Sg9lZ3VGeCkj91uoiCSGEZRp1QAe4Y2BbOvYdwTEdzMGVn1hdHCGEsEyjD+hKKf44LomNAf0J2b+Y7el5VhdJCCEs0egDOpix05OvvpVIVcirc96jqFyG1hVCND+1BnSl1Gyl1FGl1NazrFdKqVeUUnuUUpuVUn18X8zahV8yCq/NyaUlK/j74hTTL33rfMjaZUVxhBDionPUYZt/Aq8Cc86yfjTQufJnADCj8t+LKyAMW/uh3Jn2PTmrVsHqLLM8vi/8TBpLhRBNX601dK31UiC3hk1uAOZoYyUQoZRq5asCnpM+d+EfHMEu1YF3wn6O7jMZDq2H4mxLiiOEEBeTL3Lo8cDBk16nVy47g1JqmlJqrVJqbVZWlg8OfZqe47D/z1YyR8/ij0eH833IGEDDnsW+P5YQQjQwF7VRVGv9hta6n9a6X0xMzAU7zi2XtSEpIZzHliu8QTGwZ+EFO5YQQjQUvgjoh4DEk14nVC6zjN2m+PO4S8gqdrHWkYzes0hmNhJCNHm+COifA3dW9nYZCORrrTN8sN96uTQhgoev6sK72V1QpXkc27PS6iIJIcQFVZduix8AK4CuSql0pdQ9Sql7lVL3Vm7yFbAX2APMAn5xwUp7jh4Y2ZnR427FoxXzP3qbLen5VhdJCCEumFq7LWqtJ9WyXgP3+axEPjamf09K1vZhUNYGxs1czpy7+zOgQ5TVxRJCCJ9rEk+K1iao52i66z10DS3l959txeXxWl0kIYTwuWYR0Ol8FQBPX3KUlKNFvLtiv8UFEkII32seAT0uCYJbcknpaoZ2jubFRbvJKSq3ulRCCOFTzSOg22zQ6SrUnsU8cU0Co93fUjDjanihJ5TK6IxCiKaheQR0MGmXsmN0+GcyzzpmYivKgIJ02PCe1SUTQgifaD4BvdNVkHAZ9LqZotu+5Kf219jh7IleMwu80kgqhGj8mk9ADwiHqYtg3GuEdB7Cb67rwWvFI1B5aXhTZGgAIUTj13wC+mnG902g07BbyNQRpH75IqY7vRBCNF7NNqADPHhtT3a0vonOBSt4+z8yZroQonFr1gFdKcWwWx7Fgx3v6lm8+cNeq4skhBDnrVkHdABbeCtUz3Hc6reUF77awOIdmVYXSQghzkuzD+gAtgHTCPIW84sW63nggw3sziy0ukhCCHHOJKADJA6AVr35heufjHGuY+o7a8krrrC6VEIIcU4koAMoBbe8hy2mK895nuX2ore471+rZRAvIUSjIgH9uPAEmLIA+t3NNNt/uD/9Uf4w52vK3TLTkRCicZCAfjKHP4x9EcbNpL8zlT+l3cHCl6ZRdOykCa3d5XBgJeTus66cQghRjVonuGiWek/C0W4wh+b9jjEHP6Hk5QWUJU0gIC8FDq0FdxnY/eH6v0PSRKtLK4QQgNTQzy6iDW2nvsuqaz9ntacrzo3vUFh4DN3vbrj5HTMuzKfT4Jv/lQmohRANgtTQazHo8mGsiJ3H8LkbSD9cTrIjgoc6dGHYHWNQX/8Glr8CR3fAuBkQEmN1cYUQzZiyagyTfv366bVr11py7PNR4fYyb106ry3Zw6FjpYzo1pLXb+tDwKY58NWvwOaApElw+S8hqqPVxRVCNFFKqXVa637VrpOAfm7K3R7mLN/P/y3YwZBO0cy6sx8B+Xth+d9h04fgqYCOV0JwS7A7wO4H7YbAJTdZXXQhRBMgAf0CmLv2IL/+ZDNDO8fwxh19CXDaoegorPoH7PwCXKXgcYGrGMryYdxM6D3J6mILIRo5CegXyNw1B3nsk80M7xLDP44H9dO5y+G98bB/OUz6qGrCaiGEOB81BXTp5VIPEy5L5K839eL73VmMeP473l914MynSx3+MPE9iOkOc++EwxvMclcZpH4Lq2dB6bGLX3ghRJMjNXQfWJ6azXNf72LDgWMktgjkgRGdGZccj9N+0vWy8Ai8dbVJxbTuA2k/gKvErAtuCaP+YvLsSlnzIYQQjYKkXC4CrTXf7cribwt3sfVQAYktApk+vBM39Y3H31GZislOgbfHgH8IdLrazHMaEA7//bWpuXccAf2nQVEm5KdDYQZ0vQ66jbH2wwkhGgwJ6BeR1ppFO47y6rcpbErPp1V4AD8b2oGJlyUS7O8wE1LbTst0eT2w5i1Y/CRUVA7dq2zgH2oaVPtOgWv/D/yCLv4HEkI0KPUO6EqpUcDLgB14U2v9zGnrJwPPAYcqF72qtX6zpn021YB+nNaaZXuy+fu3e1i9L5fwQCe3D2zDXZe3o2VoQPVvKsqCnD0QHg+hrUF7YclT8OPLEN0Vxr8Fcb0u7gcRQjQo9QroSik7sBu4GkgH1gCTtNbbT9pmMtBPa31/XQvV1AP6ydYfyOON7/fy9fYjOG02HryqM7+4oiOqrvny1G/h03uhNA8unQiD7oeW3S5soS82rwds1fQSEkKcoqaAXpdH//sDe7TWeyt39iFwA7C9xneJKn3aRDLzjr7syy7m+a938dzXu9h2OJ/nxieZNExtOo6A6cthydOw8X3Y8C50vga6XQflhVCSY1Iz3a4zefn6SP0W9iyGkX8wPXQuhrz98OZIuOI3cNk9F+eYQjRBdamhjwdGaa2nVr6+Axhwcm28sob+FyALU5t/WGt9sJp9TQOmAbRp06bv/v37ffQxGg+tNbN+2MszC3bSJTaUWXf2I7HFOeTGi7NNvn31G1CSbZbZHOAIgIoiGPoIXPm7E7XdsgLzFGt+Oox+xjTCnk3mNnjzavMwVJdRMOFdcPid/4etqy8ehrWzwREIv1gOLTpc+GMK0UjVN+VSl4AeBRRprcuVUj8HJmqtR9S03+aUcqnO97uz+OX767HZFL8d053xfRKw2c6hy6K73HSFDIw0jafucljwKKyfA+2Hw40zYfvnsPRZU4NXdojqBLd+WH3ALMmFN64w++k/Fb59yvSwufmfFzaoFxyGl5PMHce+pdAqCe78/MyGY1E/R7aaRvW6XCzz9ptG+YjEC18ucc7q+2DRIeDkv2wCJxo/AdBa52ityytfvgn0PZ+CNifDu8Tw+f1D6BgTwmPzNjN+5nK2Hc4HoMzlYd3+PP61cj8Hckqq34HDHyLbQkCY6bvuDDDjs9/wGhxcBS/0MN0hY3vCz5bAnf+G4qMwaySk/Xjqvjxu+Pgu003ylvdg2KMw+jnY9SXMm2LSObn74OBqk45xl1dfpvOx/O8mf37t/8E1T5n++etm+27/AnJS4a1r4J3roeIs36fjSnJN+mvWCDOUhWhU6lJDd2DSKCMxgXwNcKvWettJ27TSWmdU/n4j8Gut9cCa9tvca+jHeb2a+RsO8ZevdpBXUkHXuDD2HC3E5TF/l+gQfz6cNpBOLUPqvtOMzaZnTNIk6DTyxMNKOanw/kTISzONq1EdTY1t73ew7m244XVIvu3EflbOgP8+fub+4y6Fm96EmK7n/bkB06vnpV5wyU9h3OugNbw7DtLXwi9WQESb+u3/dMU5sGUuhLYyvYUi29W9IdbrBe0Bu9O3ZbrQ3BUw+xrzDERFEQx7DEb87uzbfzrdnCObA9oMgtvnn9vdUvo6iOpg7hzFBeGLbotjgJcw3RZna62fVko9CazVWn+ulPoLcD3gBnKB6VrrnTXtUwL6qfJLXLy8OIVdmQVcmhBB78QIooL9uPdf61EKPpw2kI4x5xDUz6b0mMlZpy0zNfbjBkw3OfbT7fgCsndDSEsIiTU9bf77uKnpXfs09Lv7xAXD6zW/n95759A6WP6q6ZI58g/Q+WqzfNGfYNlLcP9aiO5kluXth9cHQXwfM5FIcFT9PzOYu4x/XgdHtpxY5gwyF7zRz0JY6+rf5yqDTe+bC6TXA3d8dqKsjcGiP8GyF017yPZ/w47/wH2roEX7M7fdsxj+9VPTDhPRBv7zoPl7DX2kbsfa9JGZ9CWwhblo9JlsRhwVPiUPFjViKZmF3PLGShx2xUfTBtEuOth3Oy8vNKmU8gJTG6trbbXwCHz2C0hdDLG9AG1uz0uywT8MWidD694Q0RY2z4UDy83yoCjI2wfJt5u0zozBJnd+89un7n/9HPj8l6aRNPk2GKoFgdIAABRESURBVPiL+o0x7yqFf91kUkYT5kBYK5NTztgEG98zte7Rz8GlE05cjPL2w7ZPYeXr5snd1n0gv7Kd/47PIO6S8y/PhVB4xAwAF9cLojubZfuWmjRLnztMOq7gMPy9H3S4Aia9f+r7y4vMhdThD/cuM//Ou9tcBKZ8BW1qvOE2abx3x0F8P/M9SvsBWvYwQ1p0uML3n9cKRVmmPammjgX1dXAN/PA8dP8J9L6t2qFAJKA3cruOFDJp1kpsSjGgfQsig520CPanVXgA3VuF0TU2lEC/i9yH2+uFNbNM0AtsAcHREBwDxVlmGIOj28HrhvA2MHC6CSp2P/j+r6ZWrmzgdcG9P1YfHI/uhBV/NxcEjws6DDfdNztcCbGXmDSAu8LcZRRnQ0WxGRvHVWL+w7XuY9oXPG746HbY/V+TJuo1/tTj5KTCZ9NNu0O3sSYdk/ot5Kaa9R2uhCEPQ/th5g5jzg0mdXH7fEio9v9UzbQ2gTWsdc3j9hRmwsGVkLXLdEWN73PmNjmppsa98wtIX3NieVQn00tp63zTEPrzpeBXWRFY9qKptd/2yakjfy54HFbNgCn/hbaDzLKyAvjHMHP+p3xpUlTVyUk1efegaJi6EAIiTLm++R0cO2DKP/IPpsG7rsoKTFvNhZwFrLzQ3IG6SyE8EcITzI9/6GllyYfvn4VVM8EZDEP/Bwbca9qtzpXHDemrIXcvJA40FRWlzDEWP2l6sNn9wFMO7YbC2JfOuCOUgN4E7Mgo4Okvd5CRX0pucQXHSl0c/9PZFLSPDmby5e24fWDbuj+wdCG5ykxtPKrzmbfdh9bDFw9By55w44ya91N4xIxIufMLyKrM4gVGAgpKc2t4ozI1RP8QE6yv+xtcNrX6Tb0eWPGa6dljs5sJSTqOgI4jIabLqdvm7TdBvego9L7V/Of3CzbpGwC0CdrOgMogkWjuCDI2m8+w80soOASR7U07xqUTzO/Zu+HACjiw0vx77LQuvYkDTBBJ7G96L235GA6vN+taJUG3n5iLXsYm2LXA1M7BBNjWySf24y43NXGl4KonTMDN3Qtr3jTn57rnTz3u4Q0we5R5X8croc+d0HWMqcFrbS6ms68xAWnqolN70bjKTPfaH/4GZcfM4HP97jYX/sBIE/hP7kHlcZuL6aYPzHnyVED7oeY8df/JiZqx12vuKouOmob8wiPmtbKZv5+ymYpFXpr5exVnmwfxEvpD4mXm771+jrnguYrP/D7EXmJ6irUfZu46Fz1h9pd8uzlmytfm7zrsUfM095Et5qc013znWiWZn8AW5mLhKjWpytRvYffXp35vw+Kh7WDz9yrKhAE/hyt/a8q28I9mQvr+PzMpz0pqyEMS0Jsaj1dzKK+U7RkF7MgoYHlqNmvS8pjQL4E/j7vkxIBgTUnBYdOAu3+5qcWExJovenA0+IWcCKxFR8yta/pqM9/rgHthyEO177+swASq2h6oKjwCH08xdyEVReZOpC4cgSZnn9jf5Kv3LQU0+IWeGMMnuCW0GWBqb20GmmC/+SNY/Q8ToI5rlQSXjIee46pvPC4rMEG0unUpC80Y/cf5hZi7jYn/OrN2Cibob3gPNvwLCtLBVtkw7HWZf+1+cNd/zp6WKT1mejOtfP3ECKPH2f3NXYRfiLnLKs01wf6S8RDUwly4cvea7QLCTtyJ1UVwjEn7BUWZgFt4+MQ6Z7BpjO9zpwmq+ekmpZa717QvHVhpaslgLgRjnj1xYdz7PXzzeziy2bz2DzOprsBI8yxH3r7qyxMYCZ2vha6jIKab+R7v+94cL6KtuZjGn9RBsPAILPg1bP/slN2oJwokoDd1Xq/mpcUpvLI4hT5tIph5R9+zjxkjfMtdcVJNr7JRuKKkMkgcgPxDphGy44gTqQ8wy7d8bAJ1wmUmILboUH0qxusxtbvs3aaGfPqdw7k6sMq0HUS0NYGzLnd1Xg+kLjFByGY3gdzuhHbDzEWoNsXZkLHRBPjSPHPBKS86NUh3HW2C3vGau9amUX3bp2Ybv2ATjP1DICQOQuNMmiwgzNSWvR7TGyko6tRzDebvcXC1qfl3u676i9dxrjJTIXCVmZTRGQPqeeHQWlOpiGhz6vkryzcXkIpicAaaC7lfMER3Ob9GYlep+WyVlH+IBPTm4qstGTwydxOhAQ5uTI5nYIco+rWLJDSgkXW3E0JUq75juYhGZEyvVrSNCuLPX2zn7R/T+MfSvdgUdIgJISLQSXigk/AgJ0M7R3N9Ujz2c3k6VQjRoEkNvQkrc3lYfyCPlak57M4sIr/URX6pi+yico4WltMtLpRHr+3KiG4tG0ZDqhCiVtLLRZzC69V8uSWDv32zi7ScEpISwokK8Se/1EVBqWno6tQyhM6xoXSNDeXyjlFEBl+EQbqEELWSgC6q5fJ4+WjNQd5bdQC7DcIDnYQFOHF7NXuOFrE/pxivhrAAB4+N6sak/m0kRSOExSSgi/NS5vKwPaOA57/exfLUHJISwnlqXC96JVzAJ+WEEDWSgC7qRWvN55sO89SXO8guKmdIp2hu6pPAtT3jLv4TqkI0cxLQhU8UlLmYvWwf89alk55XSoi/g2t7xnF5xygGdGhBQqRMYi3EhSYBXfiU16tZtS+XT9ans3B7JvmVDanxEYF0jQulRbAfUcF+RIf4k9wmgqTECJx2mbBCCF+QfujCp2w2xaCOUQzqGIXXq9mVWcjKvTms3pfL/pwSth8uILe4ggqPebot2M/OwA5RDOsSw5herYgJPfPRerfHi0OCvhD1IjV0cUForcktrmD1vlyW7cnmxz3ZpOWUYLcphnSK5sbkeEIDHKzal8uqvTlsPVxAVLAfPVuHcUl8OL3iw+nfvgURQdJdUoiTScpFNAgpmYV8tvEQn204zKFjpQD42W30TowguW0EWYXlbDtUQMrRQrzaDI/RPS6MQR2jGNI5mss7RtU46FiF28tHaw4ANJxRJ4XwMQnookHxejUbDuZR4dYkt4kgwHlqkC5zedhyKJ8VqTmsSM1h3YE8KtxeQvwdDO8awzU9YunXrgWtwwNQSqG1ZuH2TP6yYCf7ss0gWWMvbcVz45OkF45ociSgi0atzOVhRWoO32w/wsLtR8kuMsOaRgQ56dEqjAq3l7X78+jUMoTfjenOziOFPPv1Tnq0CuONO/sRHxHIwdwSftyTzd7sYvq3a8GQztFnXEiEaAwkoIsmw+vVbDmUz+ZD+Ww/nM/2wwXklbj42dD2TOrfpqph9dudmTz4wUacDhuhAQ7255jhWe02hcerCXTaGdo5mqt6xHJFlxhahslQw6JxkIAumqU9R4v438+2EuxvZ3CnaIZ0iqZNVBCr9uaycHsmC7dncqSgDICercO4omsMw7u0JLlN7d0sXR4vDpuSPL246CSgC1ENrTU7MgpZsuso3+/KYt2BPDxeTYi/g0Edo+jfroWZ7tHlodTlIaeogrScYvbnlJCRX0bP1mH8ckQnrukRh03GuBEXiQR0Ieogv9TFitRslqZks3R3Ful5pVXr7DZFZJCTtlHBtI0KIi4sgK+2ZJCWU0LnliH8fHhHkttEEB8R2OBz88Xlbpan5tCvbaSMotkISUAX4hxprckrceGwKwKd9mpTMB6v5ovNh3l9SSq7MgurlseFBRAb5k+gn50gPweBTjtKmdnUvFrj1Rq3R1Ph8eLymN47l3eMZliXGDrGBKOUIqeonG2HC9iXXUzHmBCSEsOrZp3SWnOkoIwt6fmkZhVzILeYA7klZBaUc3nHKG65rA09WoedUd7c4gr+uTyNOSvSOFbiomWoP8/fnMSwLjEX7DwK35OALsQF5PVqNqUfIy2nmAM5pRzILSGrqJyyCpOqKalwowGbUtgUKBROh8LPbsNpt3G0sLyqu2V8RCBaaw7nl51yDKWga2wosWEBbDtcUNXTB6BFsB9tWgQREeRkeWoOFW4vlyaEM7JbLGVuD8dKKsgpqmBpShZlLi9X94jl+qTWvLI4hZSjRUy+vB2Pj+5WpzsLrTVur5ahHCwkAV2IBu5gbglLU7L4cU82TrvNPDHbOpz2McGkZBaxbn8e6w/kkVVYTo/WYVwaH06vhAi6xIacMl/ssZIKPt1wiA9XH2RXZiEOmyIiyI+IICfJiRFMG9aBzrFmcuQyl4dnFuzkn8vTiAn1JybEH6fDhtOmcNpt+DnMj9OuyCmqILOgjIz8MlweL91bhdGvbSR927UgKSGchMigU8bKP1pYxorUHLYdLqC0woPL46XC7SUy2I9BHcxgbjLP7fmRgC5EM6O1ptTlqUz31Nxgu3R3Fh+tPUi5y4PLo6lwm1SQy+OlvPL3FsF+xIUHEhfmj9NuY+PBY2w8eIySCg8A/g4bHWNCaBsVxJ6jRaQcLQLAz2EjyM+OX+UFIquwnHK3F7tN0Ss+nKSEcLrEhdItLpR2UcHYlMKjNR6vJr/URXpeCel5pRw+VkZCZCBDO0fTNir4gp8/K3m9usZG9noPzqWUGgW8DNiBN7XWz5y23h+YA/QFcoCJWuu0OpVeCOFzSimC/Oo29t6wLjHnlUd3e7zsyChke0Z+VRDfnlFAu6hgbuqbwOUdo+jZOvyUmnuZy8OGA8dYkZrNir05fLL+EEXl7lqP5bAp3F5T+UxsEchl7VpgU4oyl4cylxkELizAQUiAgxB/BxVuL/mlLo6VuiipcBPk5yAswElogINAPzter7loeLQ2Fy6XuXi5vaZN4/jsXUpBel4pBysvLHaliK1sI2kZFkBMiD/RoWZk0YhAP45fO5WCAKfdTMoe6MRhU2QWlJNytJDdmUUcLSgj0M9OsJ+DIH87uUUV7DxSyM4jBaTllNAy1J9ucaF0axVG19hQ2kcH0y46mPDAmu9qav2LK6XswGvA1UA6sEYp9bnWevtJm90D5GmtOymlbgH+Ckys9a8khGi0HHYbvRLCz2kGqwCnvWqkTjB3Eul5pew6Usj+3BJsygRvu81GSICDhMhAEiIDiQ72Z19OMctSslm2J5tlKdnYbYoApx1/h8nnF5W7zU+ZGz+HrSqYBvs7yC0upaC0gIIyF2UuDzalsNsUdqWqUkv+DhsOu42iMjf5pS5KXebuIzrEj4TIIHrFh6M1ZBaUse5AHpkF5VS4vXX63H52W9Xoo2DuXE5/b5sWQXSNC+WqHrEcLShnR0YBy/Zk4/KcyKJE1dIrqS6X8P7AHq31XgCl1IfADcDJAf0G4E+Vv88DXlVKKW1VPkcI0SgopUhsEURii9onR+kYE0LHmBDuurzdhS8YZrA3r9ZnbSzWWlNQ5ia7qJzswnIKytxVyzXmbiS/1MWxEhfF5W5aRwTSOTaELrGhRAX74dVQ6vJQXO4mxN9BsP+Z4bjC7SUtp5h92cWkZReTllPM+hrKXJeAHg8cPOl1OjDgbNtord1KqXwgCsg+eSOl1DRgGkCbNm3qcGghhLCGn6PmnjxKqaq7gI4xIee8f7uCEH+TIqqpDF1iQ+lS2ZAN8MxZt4aL2vdIa/2G1rqf1rpfTIz0fRVCCF+qS0A/BCSe9Dqhclm12yilHEA4pnFUCCHERVKXgL4G6KyUaq+U8gNuAT4/bZvPgbsqfx8PfCv5cyGEuLhqzaFX5sTvB77GdFucrbXeppR6Elirtf4ceAt4Vym1B8jFBH0hhBAXUZ06qmqtvwK+Om3ZH076vQy42bdFE0IIcS5kQAYhhGgiJKALIUQTIQFdCCGaCMsG51JKFQK7LDl44xLNaQ9oiTPIOaobOU9109DPU1utdbUP8tRt9J4LY9fZRgwTJyil1sp5qpmco7qR81Q3jfk8ScpFCCGaCAnoQgjRRFgZ0N+w8NiNiZyn2sk5qhs5T3XTaM+TZY2iQgghfEtSLkII0URIQBdCiCbCkoCulBqllNqllNqjlHrcijI0NEqpRKXUEqXUdqXUNqXUg5XLWyilFiqlUir/jbS6rA2BUsqulNqglPqi8nV7pdSqyu/UR5UjgzZbSqkIpdQ8pdROpdQOpdQg+S6dSSn1cOX/t61KqQ+UUgGN+bt00QP6SXOUjgZ6AJOUUj0udjkaIDfwiNa6BzAQuK/yvDwOLNZadwYWV74W8CCw46TXfwVe1Fp3AvIw89w2Zy8D/9VadwOSMOdKvksnUUrFAw8A/bTWl2BGkz0+J3Kj/C5ZUUOvmqNUa10BHJ+jtFnTWmdorddX/l6I+Q8Yjzk371Ru9g4wzpoSNhxKqQTgOuDNytcKGIGZzxaa+XlSSoUDwzDDWqO1rtBaH0O+S9VxAIGVE/MEARk04u+SFQG9ujlK4y0oR4OllGoHJAOrgFitdUblqiNArEXFakheAh4Djk+bHgUc01q7K1839+9UeyALeLsyLfWmUioY+S6dQmt9CHgeOIAJ5PnAOhrxd0kaRRsYpVQI8AnwkNa64OR1lbNANet+pkqpscBRrfU6q8vSgDmAPsAMrXUyUMxp6RX5LkFlG8INmAtgayAYGGVpoerJioBelzlKmyWllBMTzN/TWs+vXJyplGpVub4VcNSq8jUQg4HrlVJpmHTdCEy+OKLythnkO5UOpGutV1W+nocJ8PJdOtVVwD6tdZbW2gXMx3y/Gu13yYqAXpc5SpudyjzwW8AOrfULJ606eb7Wu4B/X+yyNSRa699orRO01u0w351vtda3AUsw89lCMz9PWusjwEGlVNfKRSOB7ch36XQHgIFKqaDK/3/Hz1Oj/S5Z8qSoUmoMJg96fI7Spy96IRoYpdQQ4AdgCydyw7/F5NHnAm2A/cAErXWuJYVsYJRSVwC/0lqPVUp1wNTYWwAbgNu11uVWls9KSqnemEZjP2AvMAVTgZPv0kmUUk8AEzG9zDYAUzE580b5XZJH/4UQoomQRlEhhGgiJKALIUQTIQFdCCGaCAnoQgjRREhAF0KIJkICuhBCNBES0IUQoon4fxOoN6+DJQNVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfp = pd.DataFrame(performace.history)[['training_loss', 'validation_loss']]\n",
    "dfp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perf = performace.evaluate(trainX, trainY)\n",
    "valid_perf = performace.evaluate(validX, validY)\n",
    "test_perf = performace.evaluate(testX, testY)\n",
    "\n",
    "best_epoch = earlystop.best_epoch\n",
    "paras = model.count_params()\n",
    "\n",
    "res = {'dataset': dtype, 'model':mtype, 'trainable_paras':paras, \n",
    "       'best_epoch': best_epoch, 'lr': learning_rate, \n",
    "       'batch_size': batch_size, 'random_seed': seed,\n",
    "       'train_samples':len(train_idx),\n",
    "       'valid_samples':len(valid_idx), \n",
    "       'test_samples':len(test_idx),\n",
    "       'train_rmse':np.nanmean(train_perf[0]),\n",
    "       'valid_rmse': np.nanmean(valid_perf[0]),\n",
    "       'test_rmse':np.nanmean(test_perf[0]),\n",
    "       'train_r2':np.nanmean(train_perf[1]),\n",
    "       'valid_r2': np.nanmean(valid_perf[1]),\n",
    "       'test_r2':np.nanmean(test_perf[1]),\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6466519335827382"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['test_rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'attentiveFP',\n",
       " 'model': 'both',\n",
       " 'trainable_paras': 803681,\n",
       " 'best_epoch': 71,\n",
       " 'lr': 0.0001,\n",
       " 'batch_size': 128,\n",
       " 'random_seed': 77,\n",
       " 'train_samples': 3360,\n",
       " 'valid_samples': 420,\n",
       " 'test_samples': 420,\n",
       " 'train_rmse': 0.17260548465246406,\n",
       " 'valid_rmse': 0.6095254303377118,\n",
       " 'test_rmse': 0.6466519335827382,\n",
       " 'train_r2': 0.9790549568303953,\n",
       " 'valid_r2': 0.7613505188968319,\n",
       " 'test_r2': 0.7232214791759088}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
