{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [13:55:21] Enabling RDKit 2019.09.2 jupyter extensions\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from molmap import model as molmodel\n",
    "import molmap\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump\n",
    "tqdm.pandas(ascii=True)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "np.random.seed(123)\n",
    "tf.compat.v1.set_random_seed(123)\n",
    "\n",
    "\n",
    "tmp_feature_dir = './tmpignore'\n",
    "if not os.path.exists(tmp_feature_dir):\n",
    "    os.makedirs(tmp_feature_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1 = molmap.loadmap('../descriptor.mp')\n",
    "mp2 = molmap.loadmap('../fingerprint.mp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: Tox21 number of split times: 3\n"
     ]
    }
   ],
   "source": [
    "task_name = 'Tox21'\n",
    "from chembench import load_data\n",
    "df, induces = load_data(task_name)\n",
    "\n",
    "MASK = -1\n",
    "smiles_col = df.columns[0]\n",
    "values_col = df.columns[1:]\n",
    "Y = df[values_col].astype('float').fillna(MASK).values\n",
    "if Y.shape[1] == 0:\n",
    "    Y = Y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_name = os.path.join(tmp_feature_dir, 'X1_%s.data' % task_name)\n",
    "X2_name = os.path.join(tmp_feature_dir, 'X2_%s.data' % task_name)\n",
    "if not os.path.exists(X1_name):\n",
    "    X1 = mp1.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X1, X1_name)\n",
    "else:\n",
    "    X1 = load(X1_name)\n",
    "\n",
    "if not os.path.exists(X2_name): \n",
    "    X2 = mp2.batch_transform(df.smiles, n_jobs = 8)\n",
    "    dump(X2, X2_name)\n",
    "else:\n",
    "    X2 = load(X2_name)\n",
    "\n",
    "molmap1_size = X1.shape[1:]\n",
    "molmap2_size = X2.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weights(trainY):\n",
    "    \"\"\"pos_weights: neg_n / pos_n \"\"\"\n",
    "    dfY = pd.DataFrame(trainY)\n",
    "    pos = dfY == 1\n",
    "    pos_n = pos.sum(axis=0)\n",
    "    neg = dfY == 0\n",
    "    neg_n = neg.sum(axis=0)\n",
    "    pos_weights = (neg_n / pos_n).values\n",
    "    neg_weights = (pos_n / neg_n).values\n",
    "    return pos_weights, neg_weights\n",
    "\n",
    "\n",
    "prcs_metrics = ['MUV', 'PCBA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "patience = 50 #early stopping\n",
    "\n",
    "dense_layers = [256, 128, 64] #12 outputs\n",
    "\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "monitor = 'val_auc'\n",
    "dense_avf = 'relu'\n",
    "last_avf = None #sigmoid in loss\n",
    "\n",
    "if task_name in prcs_metrics:\n",
    "    metric = 'PRC'\n",
    "else:\n",
    "    metric = 'ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6264 783 784\n",
      "WARNING:tensorflow:From /home/shenwanxiang/anaconda3/envs/deepchem23/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "epoch: 0001, loss: 1.1750 - val_loss: 1.1569; auc: 0.6904 - val_auc: 0.7001                                                                                                    \n",
      "epoch: 0002, loss: 1.1390 - val_loss: 1.0894; auc: 0.7001 - val_auc: 0.7233                                                                                                    \n",
      "epoch: 0003, loss: 1.0897 - val_loss: 1.0297; auc: 0.7198 - val_auc: 0.7456                                                                                                    \n",
      "epoch: 0004, loss: 1.0576 - val_loss: 1.0107; auc: 0.7339 - val_auc: 0.7591                                                                                                    \n",
      "epoch: 0005, loss: 1.0361 - val_loss: 0.9877; auc: 0.7495 - val_auc: 0.7751                                                                                                    \n",
      "epoch: 0006, loss: 1.0180 - val_loss: 0.9656; auc: 0.7612 - val_auc: 0.7829                                                                                                    \n",
      "epoch: 0007, loss: 0.9996 - val_loss: 0.9546; auc: 0.7732 - val_auc: 0.7907                                                                                                    \n",
      "epoch: 0008, loss: 0.9925 - val_loss: 0.9438; auc: 0.7807 - val_auc: 0.7953                                                                                                    \n",
      "epoch: 0009, loss: 0.9758 - val_loss: 0.9507; auc: 0.7870 - val_auc: 0.7991                                                                                                    \n",
      "epoch: 0010, loss: 0.9679 - val_loss: 0.9388; auc: 0.7968 - val_auc: 0.8053                                                                                                    \n",
      "epoch: 0011, loss: 0.9600 - val_loss: 0.9246; auc: 0.8092 - val_auc: 0.8111                                                                                                    \n",
      "epoch: 0012, loss: 0.9437 - val_loss: 0.9227; auc: 0.8136 - val_auc: 0.8169                                                                                                    \n",
      "epoch: 0013, loss: 0.9324 - val_loss: 0.9111; auc: 0.8240 - val_auc: 0.8237                                                                                                    \n",
      "epoch: 0014, loss: 0.9169 - val_loss: 0.8973; auc: 0.8357 - val_auc: 0.8282                                                                                                    \n",
      "epoch: 0015, loss: 0.9016 - val_loss: 0.8812; auc: 0.8427 - val_auc: 0.8322                                                                                                    \n",
      "epoch: 0016, loss: 0.8908 - val_loss: 0.8840; auc: 0.8472 - val_auc: 0.8335                                                                                                    \n",
      "epoch: 0017, loss: 0.8755 - val_loss: 0.8889; auc: 0.8539 - val_auc: 0.8336                                                                                                    \n",
      "epoch: 0018, loss: 0.8782 - val_loss: 0.8802; auc: 0.8580 - val_auc: 0.8387                                                                                                    \n",
      "epoch: 0019, loss: 0.8517 - val_loss: 0.8656; auc: 0.8633 - val_auc: 0.8406                                                                                                    \n",
      "epoch: 0020, loss: 0.8460 - val_loss: 0.8724; auc: 0.8657 - val_auc: 0.8399                                                                                                    \n",
      "epoch: 0021, loss: 0.8267 - val_loss: 0.8822; auc: 0.8728 - val_auc: 0.8431                                                                                                    \n",
      "epoch: 0022, loss: 0.8225 - val_loss: 0.8539; auc: 0.8774 - val_auc: 0.8459                                                                                                    \n",
      "epoch: 0023, loss: 0.8063 - val_loss: 0.8542; auc: 0.8807 - val_auc: 0.8483                                                                                                    \n",
      "epoch: 0024, loss: 0.7987 - val_loss: 0.8536; auc: 0.8835 - val_auc: 0.8470                                                                                                    \n",
      "epoch: 0025, loss: 0.7888 - val_loss: 0.8423; auc: 0.8901 - val_auc: 0.8505                                                                                                    \n",
      "epoch: 0026, loss: 0.7752 - val_loss: 0.8386; auc: 0.8923 - val_auc: 0.8526                                                                                                    \n",
      "epoch: 0027, loss: 0.7583 - val_loss: 0.8828; auc: 0.8963 - val_auc: 0.8497                                                                                                    \n",
      "epoch: 0028, loss: 0.7477 - val_loss: 0.8443; auc: 0.8977 - val_auc: 0.8525                                                                                                    \n",
      "epoch: 0029, loss: 0.7395 - val_loss: 0.8333; auc: 0.9016 - val_auc: 0.8537                                                                                                    \n",
      "epoch: 0030, loss: 0.7307 - val_loss: 0.8341; auc: 0.9080 - val_auc: 0.8559                                                                                                    \n",
      "epoch: 0031, loss: 0.7147 - val_loss: 0.9012; auc: 0.9103 - val_auc: 0.8519                                                                                                    \n",
      "epoch: 0032, loss: 0.7231 - val_loss: 0.8288; auc: 0.9143 - val_auc: 0.8563                                                                                                    \n",
      "epoch: 0033, loss: 0.6949 - val_loss: 0.8304; auc: 0.9155 - val_auc: 0.8561                                                                                                    \n",
      "epoch: 0034, loss: 0.6919 - val_loss: 0.8329; auc: 0.9185 - val_auc: 0.8571                                                                                                    \n",
      "epoch: 0035, loss: 0.6868 - val_loss: 0.8363; auc: 0.9209 - val_auc: 0.8609                                                                                                    \n",
      "epoch: 0036, loss: 0.6670 - val_loss: 0.8856; auc: 0.9240 - val_auc: 0.8575                                                                                                    \n",
      "epoch: 0037, loss: 0.6592 - val_loss: 0.8471; auc: 0.9248 - val_auc: 0.8596                                                                                                    \n",
      "epoch: 0038, loss: 0.6693 - val_loss: 0.8348; auc: 0.9267 - val_auc: 0.8598                                                                                                    \n",
      "epoch: 0039, loss: 0.6527 - val_loss: 0.8333; auc: 0.9291 - val_auc: 0.8600                                                                                                    \n",
      "epoch: 0040, loss: 0.6263 - val_loss: 0.8467; auc: 0.9318 - val_auc: 0.8600                                                                                                    \n",
      "epoch: 0041, loss: 0.6389 - val_loss: 0.8203; auc: 0.9330 - val_auc: 0.8610                                                                                                    \n",
      "epoch: 0042, loss: 0.6110 - val_loss: 0.8410; auc: 0.9361 - val_auc: 0.8602                                                                                                    \n",
      "epoch: 0043, loss: 0.6004 - val_loss: 0.8459; auc: 0.9367 - val_auc: 0.8596                                                                                                    \n",
      "epoch: 0044, loss: 0.5962 - val_loss: 0.9078; auc: 0.9394 - val_auc: 0.8575                                                                                                    \n",
      "epoch: 0045, loss: 0.5965 - val_loss: 0.9296; auc: 0.9411 - val_auc: 0.8571                                                                                                    \n",
      "epoch: 0046, loss: 0.5937 - val_loss: 0.8661; auc: 0.9405 - val_auc: 0.8574                                                                                                    \n",
      "epoch: 0047, loss: 0.5766 - val_loss: 0.8896; auc: 0.9427 - val_auc: 0.8567                                                                                                    \n",
      "epoch: 0048, loss: 0.5753 - val_loss: 0.8342; auc: 0.9443 - val_auc: 0.8603                                                                                                    \n",
      "epoch: 0049, loss: 0.5629 - val_loss: 0.8701; auc: 0.9462 - val_auc: 0.8591                                                                                                    \n",
      "epoch: 0050, loss: 0.5524 - val_loss: 0.8741; auc: 0.9476 - val_auc: 0.8587                                                                                                    \n",
      "epoch: 0051, loss: 0.5469 - val_loss: 0.9202; auc: 0.9482 - val_auc: 0.8578                                                                                                    \n",
      "epoch: 0052, loss: 0.5402 - val_loss: 0.9126; auc: 0.9493 - val_auc: 0.8571                                                                                                    \n",
      "epoch: 0053, loss: 0.5351 - val_loss: 0.9062; auc: 0.9502 - val_auc: 0.8588                                                                                                    \n",
      "epoch: 0054, loss: 0.5410 - val_loss: 0.9131; auc: 0.9501 - val_auc: 0.8599                                                                                                    \n",
      "epoch: 0055, loss: 0.5183 - val_loss: 0.9032; auc: 0.9525 - val_auc: 0.8572                                                                                                    \n",
      "epoch: 0056, loss: 0.5211 - val_loss: 0.8884; auc: 0.9540 - val_auc: 0.8572                                                                                                    \n",
      "epoch: 0057, loss: 0.5075 - val_loss: 0.9459; auc: 0.9551 - val_auc: 0.8554                                                                                                    \n",
      "epoch: 0058, loss: 0.5028 - val_loss: 0.9437; auc: 0.9543 - val_auc: 0.8587                                                                                                    \n",
      "epoch: 0059, loss: 0.4965 - val_loss: 0.9784; auc: 0.9563 - val_auc: 0.8582                                                                                                    \n",
      "epoch: 0060, loss: 0.4919 - val_loss: 0.9467; auc: 0.9567 - val_auc: 0.8571                                                                                                    \n",
      "epoch: 0061, loss: 0.4921 - val_loss: 0.9564; auc: 0.9576 - val_auc: 0.8586                                                                                                    \n",
      "epoch: 0062, loss: 0.4902 - val_loss: 1.0335; auc: 0.9586 - val_auc: 0.8542                                                                                                    \n",
      "epoch: 0063, loss: 0.4775 - val_loss: 1.0974; auc: 0.9594 - val_auc: 0.8556                                                                                                    \n",
      "epoch: 0064, loss: 0.4835 - val_loss: 0.9662; auc: 0.9601 - val_auc: 0.8559                                                                                                    \n",
      "epoch: 0065, loss: 0.4808 - val_loss: 0.9672; auc: 0.9603 - val_auc: 0.8568                                                                                                    \n",
      "epoch: 0066, loss: 0.4678 - val_loss: 1.0191; auc: 0.9618 - val_auc: 0.8526                                                                                                    \n",
      "epoch: 0067, loss: 0.4680 - val_loss: 1.0101; auc: 0.9620 - val_auc: 0.8560                                                                                                    \n",
      "epoch: 0068, loss: 0.4555 - val_loss: 1.1677; auc: 0.9631 - val_auc: 0.8547                                                                                                    \n",
      "epoch: 0069, loss: 0.4539 - val_loss: 1.1313; auc: 0.9637 - val_auc: 0.8502                                                                                                    \n",
      "epoch: 0070, loss: 0.4492 - val_loss: 0.9852; auc: 0.9633 - val_auc: 0.8550                                                                                                    \n",
      "epoch: 0071, loss: 0.4561 - val_loss: 1.0966; auc: 0.9649 - val_auc: 0.8521                                                                                                    \n",
      "epoch: 0072, loss: 0.4399 - val_loss: 1.1346; auc: 0.9654 - val_auc: 0.8530                                                                                                    \n",
      "epoch: 0073, loss: 0.4442 - val_loss: 1.2461; auc: 0.9657 - val_auc: 0.8540                                                                                                    \n",
      "epoch: 0074, loss: 0.4502 - val_loss: 1.1088; auc: 0.9655 - val_auc: 0.8577                                                                                                    \n",
      "epoch: 0075, loss: 0.4327 - val_loss: 1.0720; auc: 0.9664 - val_auc: 0.8545                                                                                                    \n",
      "epoch: 0076, loss: 0.4279 - val_loss: 1.0977; auc: 0.9668 - val_auc: 0.8542                                                                                                    \n",
      "epoch: 0077, loss: 0.4273 - val_loss: 1.2240; auc: 0.9672 - val_auc: 0.8504                                                                                                    \n",
      "epoch: 0078, loss: 0.4256 - val_loss: 1.0537; auc: 0.9676 - val_auc: 0.8554                                                                                                    \n",
      "epoch: 0079, loss: 0.4253 - val_loss: 0.9854; auc: 0.9673 - val_auc: 0.8577                                                                                                    \n",
      "epoch: 0080, loss: 0.4386 - val_loss: 1.1589; auc: 0.9687 - val_auc: 0.8492                                                                                                    \n",
      "epoch: 0081, loss: 0.4216 - val_loss: 1.3293; auc: 0.9696 - val_auc: 0.8482                                                                                                    \n",
      "epoch: 0082, loss: 0.4107 - val_loss: 1.1460; auc: 0.9701 - val_auc: 0.8519                                                                                                    \n",
      "epoch: 0083, loss: 0.4070 - val_loss: 1.2928; auc: 0.9699 - val_auc: 0.8520                                                                                                    \n",
      "epoch: 0084, loss: 0.4071 - val_loss: 1.1582; auc: 0.9707 - val_auc: 0.8513                                                                                                    \n",
      "epoch: 0085, loss: 0.4001 - val_loss: 1.0656; auc: 0.9702 - val_auc: 0.8556                                                                                                    \n",
      "epoch: 0086, loss: 0.4024 - val_loss: 1.3701; auc: 0.9718 - val_auc: 0.8487                                                                                                    \n",
      "epoch: 0087, loss: 0.4009 - val_loss: 1.2780; auc: 0.9722 - val_auc: 0.8475                                                                                                    \n",
      "epoch: 0088, loss: 0.3950 - val_loss: 1.2820; auc: 0.9727 - val_auc: 0.8498                                                                                                    \n",
      "epoch: 0089, loss: 0.3912 - val_loss: 1.2952; auc: 0.9732 - val_auc: 0.8485                                                                                                    \n",
      "epoch: 0090, loss: 0.4023 - val_loss: 1.3605; auc: 0.9725 - val_auc: 0.8478                                                                                                    \n",
      "epoch: 0091, loss: 0.3952 - val_loss: 1.1334; auc: 0.9734 - val_auc: 0.8541                                                                                                    \n",
      "\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00091: early stopping\n",
      "6264 783 784\n",
      "Train on 6264 samples, validate on 783 samples\n",
      "Epoch 1/41\n",
      "6264/6264 [==============================] - 3s 423us/sample - loss: 1.1675 - val_loss: 1.1690\n",
      "Epoch 2/41\n",
      "6264/6264 [==============================] - 2s 250us/sample - loss: 1.1234 - val_loss: 1.0953\n",
      "Epoch 3/41\n",
      "6264/6264 [==============================] - 2s 250us/sample - loss: 1.0692 - val_loss: 1.0465\n",
      "Epoch 4/41\n",
      "6264/6264 [==============================] - 2s 248us/sample - loss: 1.0367 - val_loss: 1.0176\n",
      "Epoch 5/41\n",
      "6264/6264 [==============================] - 2s 244us/sample - loss: 1.0187 - val_loss: 1.0092\n",
      "Epoch 6/41\n",
      "6264/6264 [==============================] - 2s 245us/sample - loss: 1.0037 - val_loss: 0.9976\n",
      "Epoch 7/41\n",
      "6264/6264 [==============================] - 2s 249us/sample - loss: 0.9883 - val_loss: 0.9828\n",
      "Epoch 8/41\n",
      "6264/6264 [==============================] - 2s 245us/sample - loss: 0.9728 - val_loss: 0.9716\n",
      "Epoch 9/41\n",
      "6264/6264 [==============================] - 2s 250us/sample - loss: 0.9675 - val_loss: 0.9911\n",
      "Epoch 10/41\n",
      "6264/6264 [==============================] - 2s 247us/sample - loss: 0.9509 - val_loss: 0.9634\n",
      "Epoch 11/41\n",
      "6264/6264 [==============================] - 2s 248us/sample - loss: 0.9322 - val_loss: 0.9688\n",
      "Epoch 12/41\n",
      "6264/6264 [==============================] - 2s 246us/sample - loss: 0.9142 - val_loss: 0.9425\n",
      "Epoch 13/41\n",
      "6264/6264 [==============================] - 2s 247us/sample - loss: 0.9014 - val_loss: 0.9446\n",
      "Epoch 14/41\n",
      "6264/6264 [==============================] - 2s 243us/sample - loss: 0.8888 - val_loss: 0.9405\n",
      "Epoch 15/41\n",
      "6264/6264 [==============================] - 2s 243us/sample - loss: 0.8829 - val_loss: 0.9444\n",
      "Epoch 16/41\n",
      "6264/6264 [==============================] - 2s 245us/sample - loss: 0.8652 - val_loss: 0.9257\n",
      "Epoch 17/41\n",
      "6264/6264 [==============================] - 2s 242us/sample - loss: 0.8442 - val_loss: 0.9170\n",
      "Epoch 18/41\n",
      "6264/6264 [==============================] - 2s 254us/sample - loss: 0.8465 - val_loss: 0.9464\n",
      "Epoch 19/41\n",
      "6264/6264 [==============================] - 2s 248us/sample - loss: 0.8268 - val_loss: 0.9093\n",
      "Epoch 20/41\n",
      "6264/6264 [==============================] - 2s 249us/sample - loss: 0.8088 - val_loss: 0.8987\n",
      "Epoch 21/41\n",
      "6264/6264 [==============================] - 2s 249us/sample - loss: 0.7986 - val_loss: 0.9625\n",
      "Epoch 22/41\n",
      "6264/6264 [==============================] - 2s 268us/sample - loss: 0.7895 - val_loss: 0.8872\n",
      "Epoch 23/41\n",
      "6264/6264 [==============================] - 2s 249us/sample - loss: 0.7768 - val_loss: 0.9863\n",
      "Epoch 24/41\n",
      "6264/6264 [==============================] - 2s 250us/sample - loss: 0.7708 - val_loss: 0.8853\n",
      "Epoch 25/41\n",
      "6264/6264 [==============================] - 2s 260us/sample - loss: 0.7513 - val_loss: 0.8970\n",
      "Epoch 26/41\n",
      "6264/6264 [==============================] - 2s 250us/sample - loss: 0.7502 - val_loss: 0.9402\n",
      "Epoch 27/41\n",
      "6264/6264 [==============================] - 2s 247us/sample - loss: 0.7443 - val_loss: 0.8901\n",
      "Epoch 28/41\n",
      "6264/6264 [==============================] - 2s 249us/sample - loss: 0.7259 - val_loss: 0.9078\n",
      "Epoch 29/41\n",
      "6264/6264 [==============================] - 2s 257us/sample - loss: 0.7190 - val_loss: 0.8768\n",
      "Epoch 30/41\n",
      "6264/6264 [==============================] - 2s 249us/sample - loss: 0.7059 - val_loss: 0.8749\n",
      "Epoch 31/41\n",
      "6264/6264 [==============================] - 2s 253us/sample - loss: 0.7038 - val_loss: 0.8862\n",
      "Epoch 32/41\n",
      "6264/6264 [==============================] - 2s 253us/sample - loss: 0.6967 - val_loss: 0.9066\n",
      "Epoch 33/41\n",
      "6264/6264 [==============================] - 2s 247us/sample - loss: 0.6751 - val_loss: 0.9282\n",
      "Epoch 34/41\n",
      "6264/6264 [==============================] - 2s 258us/sample - loss: 0.6737 - val_loss: 0.9064\n",
      "Epoch 35/41\n",
      "6264/6264 [==============================] - 2s 245us/sample - loss: 0.6688 - val_loss: 0.8924\n",
      "Epoch 36/41\n",
      "6264/6264 [==============================] - 2s 248us/sample - loss: 0.6576 - val_loss: 0.9220\n",
      "Epoch 37/41\n",
      "6264/6264 [==============================] - 2s 249us/sample - loss: 0.6416 - val_loss: 0.9280\n",
      "Epoch 38/41\n",
      "6264/6264 [==============================] - 2s 249us/sample - loss: 0.6298 - val_loss: 0.9101\n",
      "Epoch 39/41\n",
      "6264/6264 [==============================] - 2s 246us/sample - loss: 0.6306 - val_loss: 0.9246\n",
      "Epoch 40/41\n",
      "6264/6264 [==============================] - 2s 252us/sample - loss: 0.6214 - val_loss: 0.9317\n",
      "Epoch 41/41\n",
      "6264/6264 [==============================] - 2s 249us/sample - loss: 0.6071 - val_loss: 1.0109\n",
      "6264 783 784\n",
      "Train on 6264 samples, validate on 783 samples\n",
      "Epoch 1/41\n",
      "6264/6264 [==============================] - 3s 434us/sample - loss: 1.1768 - val_loss: 1.2371\n",
      "Epoch 2/41\n",
      "6264/6264 [==============================] - 2s 257us/sample - loss: 1.1462 - val_loss: 1.1814\n",
      "Epoch 3/41\n",
      "6264/6264 [==============================] - 2s 251us/sample - loss: 1.0942 - val_loss: 1.1353\n",
      "Epoch 4/41\n",
      "6264/6264 [==============================] - 2s 256us/sample - loss: 1.0555 - val_loss: 1.1159\n",
      "Epoch 5/41\n",
      "6264/6264 [==============================] - 2s 266us/sample - loss: 1.0374 - val_loss: 1.0750\n",
      "Epoch 6/41\n",
      "6264/6264 [==============================] - 2s 261us/sample - loss: 1.0150 - val_loss: 1.0826\n",
      "Epoch 7/41\n",
      "6264/6264 [==============================] - 2s 254us/sample - loss: 1.0033 - val_loss: 1.0419\n",
      "Epoch 8/41\n",
      "6264/6264 [==============================] - 2s 254us/sample - loss: 0.9872 - val_loss: 1.0324\n",
      "Epoch 9/41\n",
      "6264/6264 [==============================] - 2s 258us/sample - loss: 0.9920 - val_loss: 1.0085\n",
      "Epoch 10/41\n",
      "6264/6264 [==============================] - 2s 256us/sample - loss: 0.9629 - val_loss: 1.0019\n",
      "Epoch 11/41\n",
      "6264/6264 [==============================] - 2s 258us/sample - loss: 0.9580 - val_loss: 0.9953\n",
      "Epoch 12/41\n",
      "6264/6264 [==============================] - 2s 254us/sample - loss: 0.9427 - val_loss: 0.9879\n",
      "Epoch 13/41\n",
      "6264/6264 [==============================] - 2s 257us/sample - loss: 0.9343 - val_loss: 0.9719\n",
      "Epoch 14/41\n",
      "6264/6264 [==============================] - 2s 259us/sample - loss: 0.9225 - val_loss: 0.9588\n",
      "Epoch 15/41\n",
      "6264/6264 [==============================] - 2s 258us/sample - loss: 0.9151 - val_loss: 0.9576\n",
      "Epoch 16/41\n",
      "6264/6264 [==============================] - 2s 256us/sample - loss: 0.9078 - val_loss: 0.9850\n",
      "Epoch 17/41\n",
      "6264/6264 [==============================] - 2s 258us/sample - loss: 0.8843 - val_loss: 0.9407\n",
      "Epoch 18/41\n",
      "6264/6264 [==============================] - 2s 256us/sample - loss: 0.8698 - val_loss: 0.9174\n",
      "Epoch 19/41\n",
      "6264/6264 [==============================] - 2s 256us/sample - loss: 0.8562 - val_loss: 0.9359\n",
      "Epoch 20/41\n",
      "6264/6264 [==============================] - 2s 260us/sample - loss: 0.8473 - val_loss: 0.9004\n",
      "Epoch 21/41\n",
      "6264/6264 [==============================] - 2s 253us/sample - loss: 0.8329 - val_loss: 0.9488\n",
      "Epoch 22/41\n",
      "6264/6264 [==============================] - 2s 254us/sample - loss: 0.8328 - val_loss: 0.9929\n",
      "Epoch 23/41\n",
      "6264/6264 [==============================] - 2s 260us/sample - loss: 0.8084 - val_loss: 0.8848\n",
      "Epoch 24/41\n",
      "6264/6264 [==============================] - 2s 255us/sample - loss: 0.8005 - val_loss: 0.8794\n",
      "Epoch 25/41\n",
      "6264/6264 [==============================] - 2s 256us/sample - loss: 0.7888 - val_loss: 0.8941\n",
      "Epoch 26/41\n",
      "6264/6264 [==============================] - 2s 253us/sample - loss: 0.7723 - val_loss: 0.8600\n",
      "Epoch 27/41\n",
      "1920/6264 [========>.....................] - ETA: 1s - loss: 0.8205"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, split_idxs in enumerate(induces):\n",
    "\n",
    "    train_idx, valid_idx, test_idx = split_idxs\n",
    "    print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "\n",
    "    trainX = (X1[train_idx], X2[train_idx])\n",
    "    trainY = Y[train_idx]\n",
    "\n",
    "    validX = (X1[valid_idx], X2[valid_idx])\n",
    "    validY = Y[valid_idx]\n",
    "\n",
    "    testX = (X1[test_idx], X2[test_idx])\n",
    "    testY = Y[test_idx]            \n",
    "\n",
    "    pos_weights, neg_weights = get_pos_weights(trainY)\n",
    "    loss = lambda y_true, y_pred: molmodel.loss.weighted_cross_entropy(y_true,y_pred, pos_weights, MASK = -1)\n",
    "\n",
    "    model = molmodel.net.DoublePathNet(molmap1_size, molmap2_size, \n",
    "                                       n_outputs=Y.shape[-1], \n",
    "                                       dense_layers=dense_layers, \n",
    "                                       dense_avf = dense_avf, \n",
    "                                       last_avf=last_avf)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #\n",
    "    #import tensorflow_addons as tfa\n",
    "    #opt = tfa.optimizers.AdamW(weight_decay = 0.1,learning_rate=0.001,beta1=0.9,beta2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer = opt, loss = loss)\n",
    "    \n",
    "    if i == 0:\n",
    "        performance = molmodel.cbks.CLA_EarlyStoppingAndPerformance((trainX, trainY), \n",
    "                                                                       (validX, validY), \n",
    "                                                                       patience = patience, \n",
    "                                                                       criteria = monitor,\n",
    "                                                                       metric = metric,\n",
    "                                                                      )\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs=epochs, verbose= 0, shuffle = True, \n",
    "              validation_data = (validX, validY), \n",
    "              callbacks=[performance]) \n",
    "\n",
    "\n",
    "    else:\n",
    "        model.fit(trainX, trainY, batch_size=batch_size, \n",
    "              epochs = performance.best_epoch + 1, verbose = 1, shuffle = True, \n",
    "              validation_data = (validX, validY)) \n",
    "            \n",
    "        performance.model.set_weights(model.get_weights())\n",
    "    \n",
    "    best_epoch = performance.best_epoch\n",
    "    trainable_params = model.count_params()\n",
    "    \n",
    "    train_aucs = performance.evaluate(trainX, trainY)            \n",
    "    valid_aucs = performance.evaluate(validX, validY)            \n",
    "    test_aucs = performance.evaluate(testX, testY)\n",
    "\n",
    "\n",
    "    final_res = {\n",
    "                     'task_name':task_name,            \n",
    "                     'train_auc':np.nanmean(train_aucs), \n",
    "                     'valid_auc':np.nanmean(valid_aucs),                      \n",
    "                     'test_auc':np.nanmean(test_aucs), \n",
    "                     'metric':metric,\n",
    "                     '# trainable params': trainable_params,\n",
    "                     'best_epoch': best_epoch,\n",
    "                     'batch_size':batch_size,\n",
    "                     'lr': lr,\n",
    "                     'weight_decay':weight_decay\n",
    "                    }\n",
    "    \n",
    "    results.append(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(performance.history)[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(performance.history)[['auc', 'val_auc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).test_auc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./results/%s.csv' % task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
