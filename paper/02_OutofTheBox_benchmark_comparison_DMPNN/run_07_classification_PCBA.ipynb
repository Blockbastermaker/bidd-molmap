{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molmap import model as molmodel\n",
    "import molmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import load, dump\n",
    "tqdm.pandas(ascii=True)\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  \n",
    "\n",
    "np.random.seed(123)\n",
    "tf.compat.v1.set_random_seed(123)\n",
    "\n",
    "#tmp_feature_dir = './tmpignore'\n",
    "tmp_feature_dir = '/raid/shenwanxiang/tempignore'\n",
    "\n",
    "if not os.path.exists(tmp_feature_dir):\n",
    "    os.makedirs(tmp_feature_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1 = molmap.loadmap('../descriptor.mp')\n",
    "mp2 = molmap.loadmap('../fingerprint.mp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset: PCBA number of split times: 3\n"
     ]
    }
   ],
   "source": [
    "task_name = 'PCBA'\n",
    "from chembench import load_data\n",
    "df, induces = load_data(task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350343 43793 43793 (437929, 129)\n"
     ]
    }
   ],
   "source": [
    "print(len(induces[0][0]), len(induces[0][1]), len(induces[0][2]), df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_idx = df[df.smiles.isna()].index.to_list()\n",
    "nan_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = -1\n",
    "smiles_col = df.columns[0]\n",
    "values_col = df.columns[1:]\n",
    "Y = df[values_col].astype('float').fillna(MASK).values\n",
    "if Y.shape[1] == 0:\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    Y = Y.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437929, 128)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 30\n",
    "xs = np.array_split(df.smiles.to_list(), batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X1_name_all = os.path.join(tmp_feature_dir, 'X1_%s.data' % (task_name))\n",
    "X2_name_all = os.path.join(tmp_feature_dir, 'X2_%s.data' % (task_name))\n",
    "\n",
    "if os.path.exists(X1_name_all) & os.path.exists(X2_name_all):\n",
    "    X1 = load(X1_name_all)\n",
    "    X2 = load(X2_name_all)\n",
    "else:\n",
    "    ## descriptors\n",
    "    X1s = []\n",
    "    for i, batch_smiles in tqdm(enumerate(xs), ascii=True):\n",
    "        ii = str(i).zfill(2)\n",
    "        X1_name = os.path.join(tmp_feature_dir, 'X1_%s_%s.data' % (task_name, ii))\n",
    "        print('save to %s' % X1_name)\n",
    "        if not os.path.exists(X1_name):\n",
    "            X1 = mp1.batch_transform(batch_smiles, n_jobs = 8)\n",
    "            X1 = X1.astype('float32')\n",
    "            dump(X1, X1_name)\n",
    "            \n",
    "        else:\n",
    "            X1 = load(X1_name)\n",
    "        X1s.append(X1)\n",
    "        del X1\n",
    "        \n",
    "    X1 = np.concatenate(X1s)\n",
    "    del X1s\n",
    "    \n",
    "    dump(X1, X1_name_all)\n",
    "    \n",
    "    ## fingerprint\n",
    "    X2s = []      \n",
    "    for i, batch_smiles in tqdm(enumerate(xs), ascii=True):\n",
    "        ii = str(i).zfill(2)\n",
    "        X2_name = os.path.join(tmp_feature_dir, 'X2_%s_%s.data' % (task_name, ii))\n",
    "        if not os.path.exists(X2_name):\n",
    "            X2 = mp2.batch_transform(batch_smiles, n_jobs = 8)\n",
    "            X2 = X2.astype('float32')\n",
    "            dump(X2, X2_name)\n",
    "            \n",
    "        else:\n",
    "            X2 = load(X2_name)\n",
    "            \n",
    "        X2s.append(X2)\n",
    "        del X2\n",
    "        \n",
    "    X2 = np.concatenate(X2s)\n",
    "    del X2s\n",
    "    dump(X2, X2_name_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437929, 37, 37, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molmap1_size = X1.shape[1:]\n",
    "molmap2_size = X2.shape[1:]\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_weights(trainY):\n",
    "    \"\"\"pos_weights: neg_n / pos_n \"\"\"\n",
    "    dfY = pd.DataFrame(trainY)\n",
    "    pos = dfY == 1\n",
    "    pos_n = pos.sum(axis=0)\n",
    "    neg = dfY == 0\n",
    "    neg_n = neg.sum(axis=0)\n",
    "    pos_weights = (neg_n / pos_n).values\n",
    "    neg_weights = (pos_n / neg_n).values\n",
    "    return pos_weights, neg_weights\n",
    "\n",
    "prcs_metrics = ['MUV', 'PCBA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 800\n",
    "patience = 10 #early stopping, dual to large computation cost, the larger dataset  set small waitig patience for early stopping\n",
    "dense_layers = [256]  #128 outputs\n",
    "\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "\n",
    "monitor = 'val_auc'\n",
    "dense_avf = 'relu'\n",
    "last_avf = None #sigmoid in loss\n",
    "\n",
    "if task_name in prcs_metrics:\n",
    "    metric = 'PRC'\n",
    "else:\n",
    "    metric = 'ROC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350343 43793 43793\n",
      "epoch: 0001, loss: 1.0686 - val_loss: 1.0003; auc: 0.0436 - val_auc: 0.0440                                                                                                    \n",
      "epoch: 0002, loss: 0.9778 - val_loss: 0.9459; auc: 0.0647 - val_auc: 0.0662                                                                                                    \n",
      "epoch: 0003, loss: 0.9321 - val_loss: 0.9517; auc: 0.0772 - val_auc: 0.0810                                                                                                    \n",
      "epoch: 0004, loss: 0.9021 - val_loss: 0.9081; auc: 0.0880 - val_auc: 0.0935                                                                                                    \n",
      "epoch: 0005, loss: 0.8781 - val_loss: 0.8900; auc: 0.0960 - val_auc: 0.1015                                                                                                    \n",
      "epoch: 0006, loss: 0.8576 - val_loss: 0.8762; auc: 0.1017 - val_auc: 0.1091                                                                                                    \n",
      "epoch: 0007, loss: 0.8391 - val_loss: 0.8735; auc: 0.1087 - val_auc: 0.1145                                                                                                    \n",
      "epoch: 0008, loss: 0.8264 - val_loss: 0.8620; auc: 0.1154 - val_auc: 0.1203                                                                                                    \n",
      "epoch: 0009, loss: 0.8118 - val_loss: 0.8528; auc: 0.1195 - val_auc: 0.1240                                                                                                    \n",
      "epoch: 0010, loss: 0.7983 - val_loss: 0.8415; auc: 0.1243 - val_auc: 0.1285                                                                                                    \n",
      "epoch: 0011, loss: 0.7841 - val_loss: 0.8362; auc: 0.1292 - val_auc: 0.1316                                                                                                    \n",
      "epoch: 0012, loss: 0.7724 - val_loss: 0.8384; auc: 0.1340 - val_auc: 0.1362                                                                                                    \n",
      "epoch: 0013, loss: 0.7605 - val_loss: 0.8358; auc: 0.1393 - val_auc: 0.1387                                                                                                    \n",
      "epoch: 0014, loss: 0.7512 - val_loss: 0.8469; auc: 0.1428 - val_auc: 0.1394                                                                                                    \n",
      "epoch: 0015, loss: 0.7386 - val_loss: 0.8328; auc: 0.1494 - val_auc: 0.1434                                                                                                    \n",
      "epoch: 0016, loss: 0.7286 - val_loss: 0.8465; auc: 0.1549 - val_auc: 0.1490                                                                                                    \n",
      "epoch: 0017, loss: 0.7179 - val_loss: 0.8238; auc: 0.1574 - val_auc: 0.1501                                                                                                    \n",
      "epoch: 0018, loss: 0.7107 - val_loss: 0.8899; auc: 0.1624 - val_auc: 0.1529                                                                                                    \n",
      "epoch: 0019, loss: 0.7034 - val_loss: 0.8459; auc: 0.1644 - val_auc: 0.1526                                                                                                    \n",
      "epoch: 0020, loss: 0.6922 - val_loss: 0.8967; auc: 0.1715 - val_auc: 0.1596                                                                                                    \n",
      "epoch: 0021, loss: 0.6830 - val_loss: 0.9374; auc: 0.1796 - val_auc: 0.1583                                                                                                    \n",
      "epoch: 0022, loss: 0.6782 - val_loss: 0.8837; auc: 0.1831 - val_auc: 0.1698                                                                                                    \n",
      "epoch: 0023, loss: 0.6685 - val_loss: 0.9283; auc: 0.1861 - val_auc: 0.1690                                                                                                    \n",
      "epoch: 0024, loss: 0.6613 - val_loss: 1.0096; auc: 0.1871 - val_auc: 0.1676                                                                                                    \n",
      "epoch: 0025, loss: 0.6575 - val_loss: 0.9072; auc: 0.1910 - val_auc: 0.1660                                                                                                    \n",
      "epoch: 0026, loss: 0.6459 - val_loss: 0.9261; auc: 0.1984 - val_auc: 0.1704                                                                                                    \n",
      "epoch: 0027, loss: 0.6441 - val_loss: 1.0073; auc: 0.2015 - val_auc: 0.1754                                                                                                    \n",
      "epoch: 0028, loss: 0.6354 - val_loss: 1.0391; auc: 0.2091 - val_auc: 0.1788                                                                                                    \n",
      "epoch: 0029, loss: 0.6297 - val_loss: 1.0680; auc: 0.2094 - val_auc: 0.1826                                                                                                    \n",
      "epoch: 0030, loss: 0.6219 - val_loss: 1.0861; auc: 0.2082 - val_auc: 0.1757                                                                                                    \n",
      "epoch: 0031, loss: 0.6202 - val_loss: 1.1453; auc: 0.2080 - val_auc: 0.1799                                                                                                    \n",
      "epoch: 0032, loss: 0.6087 - val_loss: 1.0445; auc: 0.2208 - val_auc: 0.1814                                                                                                    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sxh/Research/bidd-molmap/molmap/model/cbks.py:218: RuntimeWarning: overflow encountered in exp\n",
      "  s = 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0033, loss: 0.6074 - val_loss: 1.1066; auc: 0.2182 - val_auc: 0.1790                                                                                                    \n",
      "epoch: 0034, loss: 0.6001 - val_loss: 1.0403; auc: 0.2188 - val_auc: 0.1814                                                                                                    \n",
      "epoch: 0035, loss: 0.5976 - val_loss: 1.2078; auc: 0.2276 - val_auc: 0.1837                                                                                                    \n",
      "epoch: 0036, loss: 0.5914 - val_loss: 1.2111; auc: 0.2286 - val_auc: 0.1806                                                                                                    \n",
      "epoch: 0037, loss: 0.5890 - val_loss: 1.2877; auc: 0.2311 - val_auc: 0.1786                                                                                                    \n",
      "epoch: 0038, loss: 0.5814 - val_loss: 1.2985; auc: 0.2352 - val_auc: 0.1836                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, split_idxs in enumerate(induces):\n",
    "\n",
    "    train_idx, valid_idx, test_idx = split_idxs\n",
    "    \n",
    "    train_idx = list(set(train_idx) - set(nan_idx))\n",
    "    valid_idx = list(set(valid_idx) - set(nan_idx))\n",
    "    test_idx = list(set(test_idx) - set(nan_idx))\n",
    "    \n",
    "    \n",
    "    print(len(train_idx), len(valid_idx), len(test_idx))\n",
    "\n",
    "    trainX = (X1[train_idx], X2[train_idx])\n",
    "    trainY = Y[train_idx]\n",
    "\n",
    "    validX = (X1[valid_idx], X2[valid_idx])\n",
    "    validY = Y[valid_idx]\n",
    "\n",
    "    testX = (X1[test_idx], X2[test_idx])\n",
    "    testY = Y[test_idx]            \n",
    "\n",
    "    pos_weights, neg_weights = get_pos_weights(trainY)\n",
    "    loss = lambda y_true, y_pred: molmodel.loss.weighted_cross_entropy(y_true,y_pred, pos_weights, MASK = -1)\n",
    "\n",
    "    model = molmodel.net.DoublePathNet(molmap1_size, molmap2_size, \n",
    "                                       n_outputs=Y.shape[-1], \n",
    "                                       dense_layers=dense_layers, \n",
    "                                       dense_avf = dense_avf, \n",
    "                                       last_avf=last_avf)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #\n",
    "    #import tensorflow_addons as tfa\n",
    "    #opt = tfa.optimizers.AdamW(weight_decay = 0.1,learning_rate=0.001,beta1=0.9,beta2=0.999, epsilon=1e-08)\n",
    "    model.compile(optimizer = opt, loss = loss)\n",
    "\n",
    "    performance = molmodel.cbks.CLA_EarlyStoppingAndPerformance((trainX, trainY), \n",
    "                                                                   (validX, validY), \n",
    "                                                                   patience = patience, \n",
    "                                                                   criteria = monitor,\n",
    "                                                                   metric = metric,\n",
    "                                                                  )\n",
    "    model.fit(trainX, trainY, batch_size=batch_size, \n",
    "          epochs=epochs, verbose= 0, shuffle = True, \n",
    "          validation_data = (validX, validY), \n",
    "          callbacks=[performance]) \n",
    "\n",
    "\n",
    "    best_epoch = performance.best_epoch\n",
    "    trainable_params = model.count_params()\n",
    "    \n",
    "    train_aucs = performance.evaluate(trainX, trainY)            \n",
    "    valid_aucs = performance.evaluate(validX, validY)            \n",
    "    test_aucs = performance.evaluate(testX, testY)\n",
    "\n",
    "    final_res = {\n",
    "                     'task_name':task_name,            \n",
    "                     'train_auc':np.nanmean(train_aucs), \n",
    "                     'valid_auc':np.nanmean(valid_aucs),                      \n",
    "                     'test_auc':np.nanmean(test_aucs), \n",
    "                     'metric':metric,\n",
    "                     '# trainable params': trainable_params,\n",
    "                     'best_epoch': best_epoch,\n",
    "                     'batch_size':batch_size,\n",
    "                     'lr': lr,\n",
    "                     'weight_decay':weight_decay\n",
    "                    }\n",
    "    \n",
    "    results.append(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(performance.history)[['auc', 'val_auc']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).test_auc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_csv('./results/%s.csv' % task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
